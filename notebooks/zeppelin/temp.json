X
About
Learn
Bookmarklet
FAQ
Changelog
Contact


Paste in JSON or a URL, drop a file, , or load an  to begin.

JSON Data/URL
{"paragraphs":[{"text":"%spark2\n\nimport org.apache.spark.sql.types._\nimport org.apache.spark.sql.{Column, DataFrame, Row, SaveMode}\nimport org.apache.spark.sql.functions._\nimport scala.util.Try\nimport spark.implicits._\nimport org.apache.hadoop.fs.FileSystem\nimport sys.process._\nimport org.joda.time.format._\nimport java.sql.{Date, Timestamp}\nimport java.text.SimpleDateFormat\nimport org.apache.spark.sql.catalyst.util.DateTimeUtils\nimport org.apache.spark.sql.functions.udf\nimport org.apache.spark.sql.catalyst.ScalaReflection\nimport org.apache.spark.sql.expressions.Window\nimport java.util.Calendar\n\ndef getCurrentTime = {\n    new SimpleDateFormat(\"yyyy-MM-dd HH:mm:ss.SSS\").format(Calendar.getInstance().getTime())\n}\n\ndef getTimeDifference(startTime: String) = {\n  val format: SimpleDateFormat = new SimpleDateFormat(\"yyyy-MM-dd HH:mm:ss.SSS\");\n    val date1: java.util.Date = format.parse(startTime);\n    val date2 = format.parse(getCurrentTime);\n    val difference = date2.getTime() - date1.getTime(); \n    \"Running time - \" + (difference / 1000 / 60) + \":\" + (\"0\" + ((difference * .001) % 60 ).toInt).takeRight(2) + \".\" + (difference % 1000) \n}\n\nval bofo20Start = getCurrentTime\n\n/* Object IngestAndWrite\n*\n*  This object will encapsulate the logic for reading and writing the HDFS\n*  \n*  Methods - loadSchema, loadEntity, writeEntity\n*\n*  Functions - schemaMapper\n*/\n\nobject IngestAndWrite {\n   /*\n    * Method - LoadSchema \n    *\n    * Parameters - inKeys:String, \n    *              inDateFields:String, \n    *              inIntFields:String, \n    *              inTimestampFields:String,\n    *              inDecimalFields:String, \n    *              inAllFieldsInOrder:String\n    *\n    * Return Type -Tuple6(  Keys:List[String], \n    *                       DateFields:List[String], \n    *                       IntFields:List[String], \n    *                       TimestampFields:List[String],\n    *                       DecimalFieldsList[String], \n    *                       AllFieldsInOrderList[String] \n    *                    )\n    *\n    * This method will read in a comma delimeted string for the key fields, the date fields, integer fields, timestamp fields, decimal fields, and all the fields\n    * If there are other data types found after bofo08 then we need to add those as well if they arent already covered.  For example, Strings cover 'CHAR' and 'UNIT' so we don't need a second data type.\n    * Also, the string data type is the most comon so that is the default and does not need to be specified.  \n    */\n    def loadSchema(  inKeys: String\n        , inDateFields: String\n        , inIntFields: String\n        , inTimestampFields: String\n        , inDecimalFields: String)(inAllFieldsInOrder: String): (List[String], List[String], List[String], List[String], List[String], List[String]) = \n        {( \n      inKeys.split(\",\").toList\n      , inDateFields.split(\",\").toList\n      , inIntFields.split(\",\").toList\n      , inTimestampFields.split(\",\").toList\n      , inDecimalFields.split(\",\").toList\n      , inAllFieldsInOrder.split(\",\").toList \n        )}\n    /*\n    * Function - schemaMapper\n    * Parameters - fieldName:String\n    *              isDeltaQueue: Boolean,\n    *              keys:List[String], \n    *              dateFields:List[String], \n    *              intFields:List[String], \n    *              timestampFields:List[String],\n    *              decimalFieldsList[String]\n    *\n    * Return Type - schema:StructField\n    *\n    * Used to map the field name to the proper data type based on the the user provided strings.  \n    * This function is designed to use the output of loadSchema as the input. \n    * The function will be used to interate through a the list of fields\n    */\n    \n    def schemaMapper : ( String \n       , List[String] \n       , List[String]\n       , List[String]\n       , List[String] \n       , List[String]) => StructField = \n       ( fieldName: String\n       , keys : List[String]\n       , dateFields : List[String]\n       , intFields : List[String]\n       , timestampFields : List[String]\n       , decimalFields : List[String]\n       )              => fieldName match {\n        case x if keys.contains(x) && intFields.contains(x) => StructField(fieldName, IntegerType, nullable =  false)\n        case x if keys.contains(x) => StructField(fieldName, StringType, nullable =  true)\n        case x if intFields.contains(x) => StructField(fieldName, IntegerType, nullable =  true)\n        case _ => StructField(fieldName, StringType, nullable =  false)\n        \n        }\n        \n        \n    /*\n    * Method - LoadEntity\n    * Parameters - schema:StructType, fullyQualifiedPath:String, formatType: String\n    * \n    * Return Type - Try[DataFrame]\n    *\n    * This method takes in a string and a schema.  The schema will be from the manually create comma delimeted strings  passed to loadschema.  Then Lists of strings are used to create the schema.  \n    * The path of the load file is hardcoded for now but can be added as parameter\n    */\n     def loadEntity(schema: StructType, fullyQualifiedPath: String, formatType: String): Try[DataFrame] = Try({\n         spark.read.format(formatType)\n        .option(\"delimiter\",\"\\t\")\n        .option(\"header\", \"false\")\n        .schema(schema)\n        .load(fullyQualifiedPath)\n        })//loadEntity end\n    \n    \n    \n    /*\n    * Method - writeEntity\n    * Parameters -outputDF: DataFrame, path: String, writeMode: String, formatType: String\n    * \n    * Return Type - Unit\n    *\n    * Writes the dataframe to path given in HDFS, using the mode 'writeMode'\n    * _SUCCESS file that is created will be deleted \n    *\n    * This write to different formats*/\n    def writeEntity(outputDF: DataFrame, path: String, writeMode: String, formatType: String): Unit =  { \n         outputDF.write\n        .mode(writeMode)\n        .option(\"delimiter\",\"\\t\")\n        .option(\"header\", \"false\")\n        .format(formatType).save(path)\n        \n        val fs:FileSystem = FileSystem.get(new java.net.URI(path + \"/_SUCCESS\"), sc.hadoopConfiguration)\n        fs.delete(new org.apache.hadoop.fs.Path(path + \"/_SUCCESS\"), false)\n    }//writeEntity end\n\n    def dynamicPartitionNumber(multiplierInt: Int) = {\n        val cores = spark.sparkContext.getConf.get(\"spark.executor.cores\").toInt \n        val allExecutors = spark.sparkContext.getExecutorMemoryStatus\n        val driver = spark.sparkContext.getConf.get(\"spark.driver.host\")\n          allExecutors.filter(! _._1.split(\":\")(0).equals(driver)).toList.length * multiplierInt * cores\n    }\n\n\n\n}//end  IngestAndWrite\n\n/* Object ProcessBOF\n*\n*  This object will encapsulate the logic for processing the BOF.  The methods for each BOF will be different\n*  \n*  Methods - updateTran01, updateTran01AndSaveErrorDF, joinBof08AndTran01\n*/\nobject ProcessBOF extends Serializable {\n\n    /*\n    * Method     - unallowable\n    * Parameters - rawString: String\n    * \n    * Return Type - Boolean\n    *\n    * This method checks the string column values for any unallowable character defined in the list.\n    * If any unallowable character is found then the method will return True else returns False.\n    */\n    def unallowable: String => Boolean = (rawString: String) => {\n        \n       //Unallowable character list \n       //$!@#%^&*()_+=-{}[]|\\:\";'<>?,./~`\n        \n        \n        val list = List(\"$\",\"!\",\"@\",\"#\",\"%\",\"^\",\"&\",\"*\",\"(\",\")\",\"_\",\"+\",\"=\",\"-\",\"{\",\"}\",\"[\",\"]\",\"|\",\"\"\"\\\"\"\",\":\",\"\"\"\"\"\"\",\";\",\"'\",\"<\",\">\",\"?\",\",\",\".\",\"/\",\"~\",\"`\")\n        if (rawString == null) {\n            false\n        } else {\n            list.exists(rawString.contains)  \n        }\n    } \n\n    /*\n    val unallowableUDF = udf[Boolean, String](unallowable)\n    */\n\n    /*\n    * Method - updateDeltaQueue\n    * Parameters - deltaQueueDF: DataFrame\n    * \n    * Return Type - Try[DataFrame]\n    *\n    * This method updates the delata queue dataframe based on the transformation logic.  \n    * The transformed dataframe is returned.\n    * This needs to change completely per BOF and DQ\n    */    \n    def updateDeltaQueue(deltaQueueDF: DataFrame): Try[DataFrame] = Try({\n        deltaQueueDF\n    })\n\n    \n    \n    /*\n    * Method - massageDeltaQueueAndReturnErrorDF\n    * Parameters - deltaQueueDF: DataFrame, inDates: String, badCharacterCols: String\n    * \n    * Return Type - Try[Tuple2(DataFrame, DataFrame)]\n    *\n    * This method updates the delta queue and also creates the error dataframe.\n    * All the fields for bad character checking and dates can be passed in as two string arguemnts and they will be handled dynamically\n    * 1. DataFrame `allColumnDF` selects all columns from incoming deltaQueue DataFrame and applies functions and return new columns and old.\n    * 2. DataFrame `massagedDF`  selects only the columns that will be used for downstream processing. Dates of 'string' datatype and ErrorChecks will be left behind.\n    * 3. DataFrame `errorDF`  selects all columns including newly created error columns and old raw columns.\n    * A tuple is returned with the updated delta queue and the error dataframe.\n    */\n    spark.udf.register(\"unallowableUDF\", unallowable)\n    def massageDeltaQueueAndReturnErrorDF(deltaQueueDF: DataFrame, inDates: String, badCharacterCols: String): Try[(DataFrame, Option[DataFrame])] = Try({\n        (deltaQueueDF, None)\n    })\n    \n    \n        \n    /*\n    * Method - joinBofAndDeltaQueue\n    * Parameters - deltaQueueDF: DataFrame, bofDF: DataFrame, keys: Array[String]\n    * \n    * Return Type - Try[DataFrame]\n    *\n    * This method merges the bof and the delta queue (tran01DF), the assumption is that the keys to join are the same name of columns\n    * A dataframe is returned that is the intersect of bof:DataFrame and tran01DF:DataFrame unioned with the set difference between bof:DataFrame and tran01DF:DataFrame\n    */\n    def joinBofAndDeltaQueue(deltaQueueDF: DataFrame, bofDF: DataFrame, keys: Array[String]): Try[DataFrame] = Try({\n            \n            val joinExprs = keys\n                           .map{ case (c) => deltaQueueDF(c) <=> bofDF(c) }\n                           .reduce(_ && _)\n             \n            bofDF.join(deltaQueueDF, joinExprs, \"outer\")\n                    .select(bofDF.columns.map(c => if (!(deltaQueueDF.columns contains \"$c\")) bofDF.col(s\"$c\") else coalesce(deltaQueueDF.col(s\"$c\"), bofDF.col(s\"$c\")) as c): _*)\n\n        })\n\n}//ProcessBOF\n\n/* Object Zarixsd2\n*\n*  This object will encapsulate the logic for processing the archive index.  The keys per BOF will be different, need to think about a way to pass thme dynamically\n*  \n*  Methods - loadZarixsd2, mergeBofWithZarixsd2\n*/\nobject Zarixsd2 {\n\n   \n    /*\n    * Method - loadZarixsd2\n    * \n    * Parameters - path: String\n    * \n    * Return Type - Try[DataFrame]\n    *\n    * This will load the archive table given the strings above in the Zarixsd2 object\n    * Uses the IngestAndWrite methods loadSchema and the function schemaMapper\n    */   \n    def loadZarixsd2(path: String) : Try[DataFrame] = {\n        /*\n        * These fields below cann be hard coded because there is only one Zarixsd2.  \n        * More dataytpes need to be handled though\n        */        \n        val keysString = \"DOC_NUMBER,S_ORD_ITEM\"\n        val datesString = \"BEARCDTE\"\n        val integersString = \"\"\n        val timestampsString = \"\"\n        val decimalsString = \"\"\n        val allFieldsString = \"COL_NAME,REQUEST,DATAPAKID,PARTNO,RECORD,DOC_NUMBER,S_ORD_ITEM,BARCHKEY,BARCHOFST,SOLD_TO,BBSTNK,MATERIAL,DOC_TYPE,BAUDAT,SALESORG,BORGDNUM,BEARCDTE,RECORDMODE\" \n\n        val fieldLists = IngestAndWrite.loadSchema(keysString,datesString,integersString,timestampsString,decimalsString)(allFieldsString)\n        IngestAndWrite.loadEntity( { StructType(fieldLists._6.map(x => IngestAndWrite.schemaMapper(x, fieldLists._1, fieldLists._2, fieldLists._3, fieldLists._4, fieldLists._5)))}, path, \"csv\")\n    } \n  \n  \n  \n   /*\n    * Method - mergeBofWithZarixsd2\n    * \n    * Parameters - bofDF: DataFrame, zarixsd2DF: DataFrame, orderedColumnsString: String, keys: Array[String]\n    * \n    * Return Type - Try[DataFrame]\n    *\n    * This will pull in Z2 archive table and merge with a the respective BOF. BOF and Zarixsd2 are arguments \n    * \n    * The BEARCDTE column coming from the bof will be kept if not null and if it is Null in bof, the BEARCDTE column in \n    * zarixsd2 will be used.  So, the bof will need to have nulls, and not blanks if it will work.  If the bof will have something \n    * other than nulls, then this function needs to be changed accordingly\n    */\n    def mergeBofWithZarixsd2(bofDF: DataFrame, zarixsd2DF: DataFrame, orderedColumnsString: String, keys: Array[String]): Try[DataFrame] = Try({\n        \n        import spark.implicits._\n        \n        val joinExprs = keys.map{ case (c) => zarixsd2DF(c) <=> bofDF(c) }\n                        .reduce(_ && _) \n         \n        val orderedColumns = orderedColumnsString.split(\",\").toSeq\n        \n        \n        zarixsd2DF.join(bofDF, joinExprs ,\"rightouter\")\n                  .select(bofDF.columns.map(c => if (s\"$c\" == \"BEARCDTE\") coalesce(zarixsd2DF.col(s\"$c\"), bofDF.col(s\"$c\")).alias(\"BEARCDTE\") else bofDF.col(s\"$c\") as c): _*)\n                  .select(orderedColumns.head, orderedColumns.tail: _*)\n                  \n    })   \n    \n    \n    \n    /*This will check for something to do the archive load. The mechanism is TBD */ \n    def triggerZarixsd2Load() : Boolean = {\n        true\n    } \n    \n}","user":"hac4796","dateUpdated":"2017-12-19T17:36:53-0500","config":{"editorSetting":{"language":"scala","editOnDblClick":false},"colWidth":12,"editorMode":"ace/mode/scala","editorHide":false,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"import org.apache.spark.sql.types._\nimport org.apache.spark.sql.{Column, DataFrame, Row, SaveMode}\nimport org.apache.spark.sql.functions._\nimport scala.util.Try\nimport spark.implicits._\nimport org.apache.hadoop.fs.FileSystem\nimport sys.process._\nimport org.joda.time.format._\nimport java.sql.{Date, Timestamp}\nimport java.text.SimpleDateFormat\nimport org.apache.spark.sql.catalyst.util.DateTimeUtils\nimport org.apache.spark.sql.functions.udf\nimport org.apache.spark.sql.catalyst.ScalaReflection\nimport org.apache.spark.sql.expressions.Window\nimport java.util.Calendar\ngetCurrentTime: String\ngetTimeDifference: (startTime: String)String\nbofo20Start: String = 2017-12-19 17:37:35.376\ndefined object IngestAndWrite\ndefined object ProcessBOF\ndefined object Zarixsd2\n"}]},"apps":[],"jobName":"paragraph_1512659779235_-1693632686","id":"20171025-160023_475785152","dateCreated":"2017-12-07T10:16:19-0500","dateStarted":"2017-12-19T17:36:53-0500","dateFinished":"2017-12-19T17:37:38-0500","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:14801"},{"text":"%spark2\ngetTimeDifference(bofo20Start)\nval numPartitions = IngestAndWrite.dynamicPartitionNumber(3)\nspark.conf.set(\"spark.sql.shuffle.partitions\", numPartitions)\n\nval keys        = \"DOC_NUMBER,S_ORD_ITEM\" \nval dates       = \"\" \nval integers    = \"\"\nval timeStamps  = \"\"\nval decimals    = \"\" \n\nval orderedFields12 = \"DOC_NUMBER,S_ORD_ITEM,STS_ITM,STS_BILL,STS_PRC,STS_DEL,DOC_TYPE,COMP_CODE,LOC_CURRCY,SOLD_TO,CUST_GRP1,CUST_GRP2,CUST_GRP3,CUST_GRP4,CUST_GRP5,DEL_BLOCK,DOC_CATEG,SALES_OFF,SALES_GRP,SALESORG,DISTR_CHAN,REASON_REJ,CH_ON,ORDER_PROB,GROSS_WGT,BWAPPLNM,PROCESSKEY,BATCH,EXCHG_CRD,EANUPC,CREATEDON,CREATEDBY,CREA_TIME,BILBLK_ITM,UNIT_OF_WT,CML_CF_QTY,CML_CD_QTY,COND_UNIT,SALESDEAL,COND_PR_UN,CML_OR_QTY,SUBTOTAL_1,SUBTOTAL_2,SUBTOTAL_3,SUBTOTAL_4,SUBTOTAL_5,SUBTOTAL_6,MIN_DL_QTY,STOR_LOC,REQDEL_QTY,MATL_GROUP,MATERIAL,MAT_ENTRD,BASE_UOM,MATL_GRP_1,MATL_GRP_2,MATL_GRP_3,MATL_GRP_4,MATL_GRP_5,TAX_VALUE,NET_PRICE,NET_VALUE,NET_WT_AP,UNLD_PT_WE,BILLTOPRTY,PAYER,SHIP_TO,PROD_HIER,FORWAGENT,ITEM_CATEG,SALESEMPLY,ROUTE,SHIP_STCK,DIVISION,STAT_DATE,EXCHG_STAT,SUB_REASON,BND_IND,UPPR_BND,DENOMINTR,NUMERATOR,DENOMINTRZ,NUMERATORZ,LOWR_BND,ST_UP_DTE,REFER_DOC,REFER_ITM,PRVDOC_CTG,VOLUMEUNIT,VOLUME_AP,SALES_UNIT,SHIP_POINT,DOC_CURRCY,COST,PLANT,TARGET_QU,TARGET_QTY,TARG_VALUE,BADV_CD,BBILL_AD,BILL_TYPE,BBO_TYPE,BBILL_ST,BCAGE_CD,BCNLDDT,B_CAS,BCOMMODTY,BCONDITN,BCONT_LN,BCNTRLNBR,B_CRM,BDISTRIB,BDEMAND,BDMDPLANT,BDOCIDCD,BDODAAC,BTGT_SHIP,BEXC_INFO,B_SST,BFMS_PRG,BFUND_CD,BINVCNT,BLOA,BCLSSA,BPO_NUM,BPO_ITMNO,BMEDST_CD,BMGMT_CD,BMIPR,BMIPRNBR,BMODDT,BNS_RDD,BORGDNUM,BOWNRSHP,BPRIUSRID,BPRI_CD,BPROJ_CD,BPONBR,BPURPRIC,BPURPOSE,BRCD_NUM,BCONDCTRL,BRETNTQTY,BRICFROM,BSTDDLVDT,BSUFX_CD,BSHIPNBR,BSIG_CD,BSPLPGMRQ,BSUPP_ST,BSUPPADR,SALES_DIST,SERV_DATE,BILL_DATE,INCOTERMS,INCOTERMS2,CUST_GROUP,ACCNT_ASGN,EXCHG_RATE,TRANS_DATE,PRICE_DATE,RT_PROMO,PRODCAT,ORDER_CURR,DIV_HEAD,DOC_CATEGR,WBS_ELEMT,ORD_ITEMS,FISCVARNT,PROFIT_CTR,CO_AREA,BCONVERT,BCUSTPODT,BBSTNK,REJECTN_ST,QUOT_FROM,ORD_REASON,QUOT_TO,BILL_BLOCK,BAUDAT,RATE_TYPE,STAT_CURR,RECORDMODE,BKDMAT,BMFRPRTNO,BPROJ_NUM,BMFR_CAGE,BMFR_NAME,DOCTYPE,BITMCATGP,DSDEL_DATE,BDISCP_CD,BMATRCVDT,BMRA_QTY,BDISCPQTY,BORDER_DT,BOFBNSRDD,BOFITMCAT,BOFPRICD,BZZESD,BRIC_TO,BMOQ,BHDRCRTDT,BZZSSC,BZZDMDDN,BZZDMDSC,BZZODNPST,BZZJOKO,BZZUSECD,BZZCHRNCD,BZZPRIREF,BZZINSPCD,BZZPICKLS,BZZMATAQC,BZZRICSOS,BZZUTILCD,BZREPMATL,BUEPOS,BSTLNR,BZZSRC_CD,BRB_DTID,BFREXP_DT,BZZCUR_DT,BFLPICKUP,BCONBIDPR,BTRQTYSUN,BCONISFFL,DLV_PRIO,BSHRTTXT,BGFM_CLIN,BGFM_CONT,BGFM_CALL,BPS_POPST,BPS_POPEN,BMIPR_NBR,BGFM_MDN,BCNDQTY,BTPDDATE,BTPDQTY,BEARCDTE\"\n\nval bofCurrentPath12 = \"/data/dataset/enrichment/txn_proc/bof_o12/current/\"\n\nval zarixsd2Path = \"/data/dataset/raw/txn_proc/zarixsd2/current/\"\n\nval z2DF = Zarixsd2.loadZarixsd2(zarixsd2Path).get\n\nval zarixsd2 = z2DF.select(\"DOC_NUMBER\",\"S_ORD_ITEM\",\"BEARCDTE\")\n\n//For each entity, BOF and DeltaQueue should share the schema\nval bofGenericFieldLists = IngestAndWrite.loadSchema ( keys,  \n                                                        dates, \n                                                        integers, \n                                                        timeStamps, \n                                                        decimals) _\n    \n\nval bofFieldLists  = bofGenericFieldLists(orderedFields12)\nval bofSchema : StructType = StructType(bofFieldLists._6.map(x => IngestAndWrite.schemaMapper(x,\n                                                                    bofFieldLists._1, \n                                                                    bofFieldLists._2, \n                                                                    bofFieldLists._3, \n                                                                    bofFieldLists._4, \n                                                                    bofFieldLists._5)))\nval bofo12 = IngestAndWrite.loadEntity(bofSchema, bofCurrentPath12, \"csv\").get.select(\"DOC_NUMBER\",\"S_ORD_ITEM\",\"BFMS_PRG\",\"BSUPPADR\",\"BORDER_DT\",\"MIN_DL_QTY\",\"BASE_UOM\",\"LOWR_BND\",\"DOC_CURRCY\",\"BPURPRIC\",\"CML_CF_QTY\",\"CREATEDON\",\"NET_VALUE\",\"MATERIAL\",\"SUBTOTAL_3\",\"SUBTOTAL_4\",\"SUBTOTAL_5\",\"SUBTOTAL_6\",\"SUBTOTAL_2\",\"SUBTOTAL_1\",\"SOLD_TO\",\"REQDEL_QTY\",\"BCUSTPODT\",\"BBSTNK\",\"CML_OR_QTY\",\"DSDEL_DATE\",\"BDODAAC\",\"ITEM_CATEG\",\"REASON_REJ\",\"LOC_CURRCY\",\"UPPR_BND\",\"SALES_UNIT\",\"BITMCATGP\",\"COND_PR_UN\",\"NET_PRICE\",\"BPONBR\",\"DOC_TYPE\",\"BMATRCVDT\",\"BNS_RDD\",\"BORGDNUM\",\"BPRI_CD\",\"BOFBNSRDD\",\"BOFITMCAT\",\"BOFPRICD\",\"BMRA_QTY\",\"CUST_GRP3\",\"BZZESD\",\"DOC_CATEG\",\"CML_CD_QTY\")\n\nval bofOrderedFieldsdfIPG = \"BCNFGIPG,OBJVERS,CHANGED,BPRICODE,BCNFCHR01\"\n\nval bofCurrentPathdfIPG      = \"/data/dataset/enrichment/txn_proc/bcnfgipg/current/\"\n\nval bofFieldListsdfIPG  = bofGenericFieldLists(bofOrderedFieldsdfIPG)\nval bofSchema : StructType = StructType(bofFieldListsdfIPG._6.map(x => IngestAndWrite.schemaMapper(x,\n                                                                    bofFieldLists._1, \n                                                                    bofFieldLists._2, \n                                                                    bofFieldLists._3, \n                                                                    bofFieldLists._4, \n                                                                    bofFieldLists._5)))\nval bofoIPG = IngestAndWrite.loadEntity(bofSchema, bofCurrentPathdfIPG, \"csv\").get\n\nval bofOrderedFieldsdfMPC = \"BCNFGBMPC,OBJVERS,CHANGED,BACQADVCD,MATL_GROUP,BBAGITMID,BDEPOTFLG,BFRGNMLTY,BCNFCHR15\"\n\nval bofCurrentPathdfMPC      = \"/data/dataset/enrichment/txn_proc/bcnfgbmpc/current/\"\n\nval bofFieldListsdfMPC  = bofGenericFieldLists(bofOrderedFieldsdfMPC)\nval bofSchema : StructType = StructType(bofFieldListsdfMPC._6.map(x => IngestAndWrite.schemaMapper(x,\n                                                                    bofFieldLists._1, \n                                                                    bofFieldLists._2, \n                                                                    bofFieldLists._3, \n                                                                    bofFieldLists._4, \n                                                                    bofFieldLists._5)))\nval bofoMPC = IngestAndWrite.loadEntity(bofSchema, bofCurrentPathdfMPC, \"csv\").get\n\nval bofOrderedFieldsdfORDT = \"BCNFGORDT,RSOBJVERS,RSRCHAGEFLAG,BMATCATGP,ITEM_CATEG,BCNFCHR01\"\n\nval bofCurrentPathdfORDT      = \"/data/dataset/enrichment/txn_proc/bcnfgordt/current/\"\n\nval bofFieldListsdfORDT  = bofGenericFieldLists(bofOrderedFieldsdfORDT)\nval bofSchema : StructType = StructType(bofFieldListsdfORDT._6.map(x => IngestAndWrite.schemaMapper(x,\n                                                                    bofFieldLists._1, \n                                                                    bofFieldLists._2, \n                                                                    bofFieldLists._3, \n                                                                    bofFieldLists._4, \n                                                                    bofFieldLists._5)))\nval bofoORDT = IngestAndWrite.loadEntity(bofSchema, bofCurrentPathdfORDT, \"csv\").get\n\nval bofOrderedFieldsdfMM = \"MATERIAL,OBJVERS,CHANGED,AF_COLOR,AF_FCOCO,AF_GENDER,AF_GRID,AF_STYLE,APO_PROD,BASE_UOM,BASIC_MATL,BBP_PROD,COMPETITOR,CONT_UNIT,CREATEDON,CRM_PROD,DIVISION,EANUPC,EXTMATLGRP,GROSS_CONT,GROSS_WT,HC_AGENT1,HC_AGENT2,HC_AGENT3,HC_ANESIND,HC_APPRTYP,HC_ATCCODE,HC_ATCMTYP,HC_CATIND1,HC_CATIND2,HC_CATIND3,HC_HAZMIND,HC_IMPMIND,IND_SECTOR,LOGSYS,MANUFACTOR,MANU_MATNR,MATL_CAT,MATL_GROUP,MATL_TYPE,MSA_USAGE,NET_CONT,NET_WEIGHT,PO_UNIT,PROD_HIER,RT_CONFMAT,RT_PRBAND,RT_PRRULE,RT_SEASON,RT_SEAYR,RT_SUPS,SIZE_DIM,STD_DESCR,UCCERTIFTY,UCCONSTCLA,UCFUNCCLAS,UNIT_DIM,UNIT_OF_WT,VOLUME,VOLUMEUNIT,BACQMTDCD,BACQMTDSX,BACQADVCD,BRTNMTDCD,BITMCNSRT,BSVCUCARM,BITMSTDCD,BITMNAMCD,BPRCMTLCD,BDEMILCD,BTECHCONT,BQUALCONT,BPACKCONT,BDEMNDPLR,BSUPLYPLR,BADPCODE,BENVATRCD,BHZDMATCD,BABCCLASS,BREPCHIND,BCIIC,BSVCUCNAV,BSVCUCAF,BSVCUCMAR,BSVCUCOTH,BPKGREVCD,BTECHRVCD,BSPLITMCD,BICC,BSRGINDCD,BSMRECVY,BPRIORRTE,BMNDSRCCD,BSTDSICCD,BITMCLSCD,BCTLACTCD,BSUBSID,BMATLSFST,BPROCGRCD,BADMLDTME,BTECHRVDT,BBAGITMID,BREPLNSN,BSOLSRCCD,BSOLSRCDT,BRATPGMID,BCOVDURTN,BPRODLDTM,BMTDCMPCD,BPROJCODE,BWEIFLIS,BSUBITMCD,BVOLFLIS,BMATLLOCK,BRELCONFM,B_PGC,BMGTGRPCD,CURRENCY,PRICE_STD,BUNITCUBE,MATL_GRP_4,B_NSN,B_NIIN,BPRCGRCD5,BSPCCODE,BMFCSTBSW,BMCLABSW,BMLEADTIM,BMZZWSIC,BMANUCOST,BMFSC,BMANU_UOM,BMATLDESC,BMDOMNTSW,BMANUEXDT,BMCONFCST,BMDISTCHN,BMANUPRCE,BMCONHIST,BMZZWSDC,BZZWSDC,BZZWSIC,BZZWSEC,BZZWSGC,CO_AREA,PROFIT_CTR,BMTLPRGRP,BFCST_IND,BIPT_CC,BPRCGRCD2,BFLIGHTSF,BLIFESUPT,BPLACEINS,BQLTYCTRL,BZZANAFLG,BZZLTCSTA,BZZSMSDRV,BZZSMSFLG,BLOGLOSSD,BMSUPCLAB,BMGFM,BMSHLFTYP,BADMDFRQ,BADMDQTY,BOWRMR,BITMCATGP,BREPMTDCD,DEL_FLAG,BOLD_MATL,BCRIT_IND,PRICE_AVG,BPR00_PRC,HEIGHT,LENGHT,LOC_CURRCY,RPA_WGH1,RPA_WGH2,RPA_WGH3,RPA_WGH4,RTPLCST,RT_COLOR,RT_FASHGRD,RT_MDRELST,RT_SEAROLL,RT_SEASYR,RT_SIZE,VENDOR,WIDTH,UNIT,BCONTRREQ,BQMPROCAT,BCRSDSTST,BCRSPLTST,BLTCAWDST,BLTCPROJS,BPRIORFYA,B2PRIORFY,BBASEPIIN,BBWPDATE,BSRC_PCMT,B_LTCAWDS,B_LTCPROJ,B_PRIORFY,B_2PRIORF,BAWD_SCHN,BAWD_SITE,BDCS_MATS,BDCS_MSVD,BDELG_PLT,BEX_STREV,BIND_COLL,BMDS,BMDS_EXDT,BMDS_LCD,BPROC_TYP,BSPE_PRTY,BXDCMS_VD,BXPMS_VDT,DOC_TYPE,DOC_NUM,BDOC_VERS,BMUNITCST,BMBDC,PRICE_MAT,SALES_UNIT,BZBPR_PRC,B_SALK3,B_LBKUM,BKEYCODE,BPRICEUNT,BKZEFF,BPEINH,BSOLSRCDA,BREPPRICE,BCUMMINSS,BCUMMAXOH,BAGPROTLV,BIADOCCAG,BIADOCNBR,BIAPCENBR,BIAREVNBR,BIACNVFAC,BZUMNFLAG,BCOG,B_SMIC,BMCC,BSMCC,BIASLACD,BIAMANAAC,BACTIVSKU,BAFERRCCD,BMCRCCODE,BARCCODE,BRETITMSW,BIAUOM,BKITENDIT,BKITCMPIN,B_CHG_ON,BZZ_LOB,BNRGMATTP,BLUST_TAX,BGAINLOSS,BCHEMCON,BQASCON,BMATWRT,BEXCLRSN,BNRECPRM,BPBL_TYPE\"\n\nval bofCurrentPathdfMM      = \"/data/dataset/enrichment/txn_proc/0material/current/\"\n\nval bofFieldListsdfMM  = bofGenericFieldLists(bofOrderedFieldsdfMM)\nval bofSchema : StructType = StructType(bofFieldListsdfMM._6.map(x => IngestAndWrite.schemaMapper(x,\n                                                                    bofFieldLists._1, \n                                                                    bofFieldLists._2, \n                                                                    bofFieldLists._3, \n                                                                    bofFieldLists._4, \n                                                                    bofFieldLists._5)))\nval bofoMM = IngestAndWrite.loadEntity(bofSchema, bofCurrentPathdfMM, \"csv\").get\n\nval bofOrderedFieldsdfDEPMAIN = \"BDEPMAIN,OBJVERS,CHANGED\"\n\nval bofCurrentPathdfDEPMAIN      = \"/data/dataset/enrichment/txn_proc/bdepmain/current/\"\n\nval bofFieldListsdfDEPMAIN  = bofGenericFieldLists(bofOrderedFieldsdfDEPMAIN)\nval bofSchema : StructType = StructType(bofFieldListsdfDEPMAIN._6.map(x => IngestAndWrite.schemaMapper(x,\n                                                                    bofFieldLists._1, \n                                                                    bofFieldLists._2, \n                                                                    bofFieldLists._3, \n                                                                    bofFieldLists._4, \n                                                                    bofFieldLists._5)))\nval dfDEPMAIN = IngestAndWrite.loadEntity(bofSchema, bofCurrentPathdfDEPMAIN, \"csv\").get\n\n\n\nval bofOrderedFields13 = \"DOC_NUMBER,S_ORD_ITEM,SALES_UNIT,RECORDMODE,BTKILLUOM,BTKILLQTY,BTVRSTCD,BTSVCCD,BTLVLAPKG,BTCONTPO,BTCCTRLNO,BTBUNDLES,BTPRCSIZE,BTRCSTCTR,BTCRID,BTSECONCD,BTCSHPLOC,BTORDTYPE,BTORDQTY,BTCORLSTN,BTCUSTCNO,BTPVACTNO,BTTRANSIN,BTTREATED,BTPRODID,BTWEAPSYS,BTPVPNO,BTPRCLIST,BTPVPRDID,BTPRCIND,BTBINLOC,BTIMESTMP,BCNT_NBR,BTVRAPDT,BEARCDTE\"\n\n\nval bofCurrentPath13      = \"/data/dataset/enrichment/txn_proc/bof_o13/current/\"//\"/user/ahung/bofo13\"//\n\n\nval bofFieldLists  = bofGenericFieldLists(bofOrderedFields13)\nval bofSchema : StructType = StructType(bofFieldLists._6.map(x => IngestAndWrite.schemaMapper(x,\n                                                                    bofFieldLists._1, \n                                                                    bofFieldLists._2, \n                                                                    bofFieldLists._3, \n                                                                    bofFieldLists._4, \n                                                                    bofFieldLists._5)))\nval bofo13 = IngestAndWrite.loadEntity(bofSchema, bofCurrentPath13, \"csv\").get.selectExpr(\"DOC_NUMBER\",\"S_ORD_ITEM\",\"BTORDQTY\",\"BTKILLQTY\",\"BTKILLUOM\",\"BTVRSTCD\", \"SALES_UNIT AS BTSALEUNT\")\n\nval bofOrderedFields14 =\"DOC_NUMBER,S_ORD_ITEM,SCHED_LINE,STORNO,REJECTN_ST,STS_ITM,STS_BILL,STS_PRC,STS_DEL,CONF_QTY,CORR_QTY,REQ_DATE,SCHD_CATEG,LOAD_DATE,DLV_BLOCK,REQ_QTY,MATAV_DATE,BASE_UOM,TRNSD_DATE,SALES_UNIT,GI_DATE,DSDEL_DATE,ORDER_QTY,BBILL_AD,BBO_TYPE,BBILL_ST,BCNDQTY,BDMDQTY,UNIT,BMATRCVDT,BSUFX_CD,BSUPP_ST,QUOT_FROM,DOC_TYPE,ORD_REASON,QUOT_TO,COMP_CODE,BILL_BLOCK,LOC_CURRCY,SOLD_TO,RATE_TYPE,CUST_GRP1,CUST_GRP2,CUST_GRP3,CUST_GRP4,CUST_GRP5,DEL_BLOCK,STAT_CURR,DOC_CATEG,SALES_OFF,SALES_GRP,SALESORG,DISTR_CHAN,REASON_REJ,CH_ON,ORDER_PROB,BATCH,EXCHG_CRD,EANUPC,CREATEDON,CREATEDBY,CREA_TIME,BILBLK_ITM,UNIT_OF_WT,COND_UNIT,COND_PR_UN,STOR_LOC,MATL_GROUP,MATERIAL,MAT_ENTRD,MATL_GRP_1,MATL_GRP_2,MATL_GRP_3,MATL_GRP_4,MATL_GRP_5,NET_PRICE,UNLD_PT_WE,BILLTOPRTY,PAYER,SHIP_TO,PROD_HIER,FORWAGENT,ITEM_CATEG,SALESEMPLY,ROUTE,DIVISION,STAT_DATE,EXCHG_STAT,SUB_REASON,DENOMINTR,NUMERATOR,DENOMINTRZ,NUMERATORZ,ST_UP_DTE,PRVDOC_CTG,VOLUMEUNIT,SHIP_POINT,DOC_CURRCY,PLANT,TARGET_QU,BADV_CD,BILL_TYPE,BCAGE_CD,BCNLDDT,B_CAS,PROFIT_CTR,BCONDITN,BCONT_LN,BCNTRLNBR,B_CRM,BDISTRIB,BDEMAND,BDMDPLANT,BDOCIDCD,BDODAAC,BTGT_SHIP,BEXC_INFO,B_SST,CO_AREA,BFMS_PRG,BFUND_CD,BINVCNT,BLOA,BCLSSA,BPO_NUM,BPO_ITMNO,BMEDST_CD,BMGMT_CD,BMIPR,BMIPRNBR,BMODDT,BNS_RDD,BORGDNUM,BOWNRSHP,BPRIUSRID,BPRI_CD,BPROJ_CD,BPONBR,BPURPRIC,BPURPOSE,BRCD_NUM,BCONDCTRL,BRETNTQTY,BRICFROM,BSTDDLVDT,BSHIPNBR,BSIG_CD,BSPLPGMRQ,BSUPPADR,SALES_DIST,SERV_DATE,BILL_DATE,INCOTERMS,INCOTERMS2,CUST_GROUP,ACCNT_ASGN,EXCHG_RATE,TRANS_DATE,PRICE_DATE,BPURREQNO,RECORDMODE,GIS_QTY,BZREPMATL,BEARCDTE\"\n\n\nval bofCurrentPath14      = \"/data/dataset/enrichment/txn_proc/bof_o14/current/\"\n\nval bofFieldLists  = bofGenericFieldLists(bofOrderedFields14)\nval bofSchema : StructType = StructType(bofFieldLists._6.map(x => IngestAndWrite.schemaMapper(x,\n                                                                    bofFieldLists._1, \n                                                                    bofFieldLists._2, \n                                                                    bofFieldLists._3, \n                                                                    bofFieldLists._4, \n                                                                    bofFieldLists._5)))\nval bofo14 = IngestAndWrite.loadEntity(bofSchema, bofCurrentPath14, \"csv\").get.select(\"DOC_NUMBER\",\"S_ORD_ITEM\",\"SCHED_LINE\",\"BASE_UOM\",\"DOC_CURRCY\",\"CONF_QTY\",\"REQ_QTY\",\"BDMDQTY\",\"SALES_UNIT\",\"UNIT\",\"DLV_BLOCK\",\"BNS_RDD\")\n\nval bofOrderedFields15 =\"DOC_NUMBER,S_ORD_ITEM,SCHED_LINE,DELIV_NUMB,DELIV_ITEM,REQU_QTY,CONF_QTY,DLV_QTY,SALES_UNIT,DLV_STSO,DLV_STS,GI_STS,LW_GISTS,DSDEL_DATE,DSDEL_TIME,CONF_DATE,CONF_TIME,PLD_GI_DTE,ACT_GI_DTE,LST_A_GD,ACT_DL_DTE,ACT_DL_TME,LST_A_DD,LST_A_DT,DLV_BLOCKD,BASE_UOM,DENOMINTR,NUMERATOR,DOC_CATEG,SCHED_DEL,REC_DELETD,BMATL_BLK,DLVQEYCR,DLVQEYSC,DLVQLECR,DLVQLESC,BCOUNTER,RECORDMODE,BCNDQTY,BEARCDTE\"\n\n\nval bofCurrentPath15      = \"/data/dataset/enrichment/txn_proc/bof_o15/current/\"\n\nval bofFieldLists  = bofGenericFieldLists(bofOrderedFields15)\nval bofSchema : StructType = StructType(bofFieldLists._6.map(x => IngestAndWrite.schemaMapper(x,\n                                                                    bofFieldLists._1, \n                                                                    bofFieldLists._2, \n                                                                    bofFieldLists._3, \n                                                                    bofFieldLists._4, \n                                                                    bofFieldLists._5)))\nval bofo15 = IngestAndWrite.loadEntity(bofSchema, bofCurrentPath15, \"csv\").get.select(\"DOC_NUMBER\",\"S_ORD_ITEM\",\"BCNDQTY\",\"DLV_QTY\",\"GI_STS\",\"CONF_QTY\",\"RECORDMODE\",\"REC_DELETD\",\"REQU_QTY\",\"DLVQEYCR\",\"DLVQEYSC\",\"DLVQLECR\",\"DLVQLESC\")\n\n\n\nval bofOrderedFields17 =\"DELIV_NUMB,DELIV_ITEM,REFER_DOC,REFER_ITM,DLV_QTY,SALES_UNIT,CREATEDON,SHIP_DATE,DEL_TYPE,GI_DATE,ITEM_CATEG,ITM_TYPE,MATERIAL,MATL_GROUP,PLANT,DOC_CATEG,SHIP_TO,SOLD_TO,ACT_DL_QTY,BASE_UOM,RECORDMODE,ACT_GI_DTE,LOAD_DATE,BILLTOPRTY,CH_ON,CREA_TIME,PROCESSKEY,GOODSMV_ST,DLV_STSOI,BSHIPTYP,STORNO,PRVDOC_CTG,STOR_LOC,BEARCDTE\"\nval bofCurrentPath17      = \"/data/dataset/enrichment/txn_proc/bof_o17/current/\"\n\nval bofFieldLists  = bofGenericFieldLists(bofOrderedFields17)\nval bofSchema : StructType = StructType(bofFieldLists._6.map(x => IngestAndWrite.schemaMapper(x,\n                                                                    bofFieldLists._1, \n                                                                    bofFieldLists._2, \n                                                                    bofFieldLists._3, \n                                                                    bofFieldLists._4, \n                                                                    bofFieldLists._5)))\nval bofo17 = IngestAndWrite.loadEntity(bofSchema, bofCurrentPath17, \"csv\").get//.select(\"REFER_DOC\",\"REFER_ITM\",\"LOAD_DATE\",\"DLV_QTY\",\"BSHIPTYP\",\"ACT_GI_DTE\",\"CREATEDON\",\"RECORDMODE\",\"PRVDOC_CTG\")\n\nval bofOrderedFields20 =\"DOC_NUMBER,S_ORD_ITEM,BFMS_PRG,BFMSCSCD,BFMS_IND,BMILSVC_1,BMILSVC_2,BMILSVC_G,BMILSVC_S,BORDER_DT,MIN_DL_QTY,BASE_UOM,LOWR_BND,DOC_CURRCY,BPURPRIC,RTPLCST,CML_CF_QTY,BTORDQTY,BTKILLQTY,BTKILLUOM,BTVRSTCD,CREATEDON,CONF_QTY,NET_VALUE,REQ_QTY,BCNDQTY,BDMDQTY,GIS_QTY,BCONVERT,MATERIAL,BIPG,SUBTOTAL_3,SUBTOTAL_4,SUBTOTAL_5,SUBTOTAL_6,SUBTOTAL_2,SUBTOTAL_1,QCOASREQ,LOAD_DATE,SOLD_TO,DLV_QTY,REQDEL_QTY,BGR_DATE,RECORDMODE,BCUSTPODT,BNS_RDD,BORD_TYPE,BBSTNK,BORGDNUM,BORGNLDOC,BPRI_CD,CML_OR_QTY,DSDEL_DATE,BMPC,ITEM_CATEG,REASON_REJ,LOC_CURRCY,UPPR_BND,SALES_UNIT,GR_QTY,BSHIPTYP,BITMCATGP,BTSALEUNT,COND_PR_UN,NET_PRICE,BPONBR,ACT_GI_DTE,DOC_TYPE,PO_UNIT,UNIT,PCONF_QTY,CML_CD_QTY,CONF_DATE,BCAGECDPN,BIDNLF,BMFRPN,DLVQEYCR,DLVQEYSC,DLVQLECR,BMATRCVDT,BNMCS_IND,BCASREP,DLVQLESC,BCREATEDT,DOC_DATE,B_YOBLOCK,B_ZTBLOCK,BOFBNSRDD,BOFITMCAT,BOFPRICD,BPLT,BACQADVCD,SCL_DELDAT,B_ALT,DLV_BLOCK,BMRA_QTY,CUST_GRP3,BZZESD,BEARCDTE\"\n\n\n\nval bofCurrentPath20 = \"/data/dataset/enrichment/txn_proc/bof_o20/current/\"\n\nval bofFieldLists  = bofGenericFieldLists(bofOrderedFields20)\nval bofSchema : StructType = StructType(bofFieldLists._6.map(x => IngestAndWrite.schemaMapper(x,\n                                                                    bofFieldLists._1, \n                                                                    bofFieldLists._2, \n                                                                    bofFieldLists._3, \n                                                                    bofFieldLists._4, \n                                                                    bofFieldLists._5)))\n//val bofo20 = try{IngestAndWrite.loadEntity(bofSchema, bofCurrentPath20, \"csv\")}\n\nval bofo20 = IngestAndWrite.loadEntity(bofSchema, bofCurrentPath20, \"csv\").get\n\n\n//val bofOrderedFields21 =\"BPO_NUM,BPO_ITMNO,RECORDMODE,GR_QTY,SCL_DELDAT,BASE_UOM,BPR_RELDT,BCONTRACT,BMOD_NUM,BCALL_NUM,B_ALT,BPLT,PSTNG_DATE,DOC_NUMBER,SCHED_LINE,S_ORD_ITEM,PO_UNIT,PCONF_QTY,CONF_DA//,BCAGECDPN,BIDNLF,BMFRPN,PRICE_UNIT,BCREATEDT,DOC_DATE,BCALL_13\"\n//val bofCurrentPath21      = \"/data/dataset/enrichment/txn_proc/bof_o21/current\" \n\n//val bofFieldLists  = bofGenericFieldLists(bofOrderedFields21)\n//val bofSchema : StructType = StructType(bofFieldLists._6.map(x => IngestAndWrite.schemaMapper(x,\n//                                                                    bofFieldLists._1, \n//                                                                    bofFieldLists._2, \n//                                                                    bofFieldLists._3, \n//                                                                    bofFieldLists._4, \n//                                                                    bofFieldLists._5)))\n//val bofo21 = IngestAndWrite.loadEntity(bofSchema, bofCurrentPath21, \"csv\").get.select(\"DOC_NUMBER\",\"S_ORD_ITEM\",\"PSTNG_DATE\",\"GR_QTY\",\"PCONF_QTY\",\"CONF_DATE\",\"BCAGECDPN\",\"BIDNLF\",\"BMFRPN\",\"BCREATEDT\",\"BPLT\",\"B_ALT\",\"SCL_DELDAT\",\"RECORDMODE\",\"SCHED_LINE\",\"PO_UNIT\")*/\n\nval bofo21 = Seq((\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\")).toDF(\"DOC_NUMBER\",\"S_ORD_ITEM\",\"PSTNG_DATE\",\"GR_QTY\",\"PCONF_QTY\",\"CONF_DATE\",\"BCAGECDPN\",\"BIDNLF\",\"BMFRPN\",\"BCREATEDT\",\"BPLT\",\"B_ALT\",\"SCL_DELDAT\",\"RECORDMODE\",\"SCHED_LINE\",\"PO_UNIT\")\n/*\n  \n//For archive?\n  \nval bofo20 = sqlContext.read.format(\"csv\")\n    .option(\"header\", \"true\")\n    .option(\"inferSchema\", \"true\")\n    .load(\"/lchu/bofo20_custom_testers/bofo20_custom.csv\")\n*/\n\n","user":"hac4796","dateUpdated":"2017-12-19T17:36:53-0500","config":{"editorSetting":{"language":"scala","editOnDblClick":false},"colWidth":12,"editorMode":"ace/mode/scala","editorHide":false,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"res10: String = Running time - 0:03.174\nnumPartitions: Int = 477\nkeys: String = DOC_NUMBER,S_ORD_ITEM\ndates: String = \"\"\nintegers: String = \"\"\ntimeStamps: String = \"\"\ndecimals: String = \"\"\norderedFields12: String = DOC_NUMBER,S_ORD_ITEM,STS_ITM,STS_BILL,STS_PRC,STS_DEL,DOC_TYPE,COMP_CODE,LOC_CURRCY,SOLD_TO,CUST_GRP1,CUST_GRP2,CUST_GRP3,CUST_GRP4,CUST_GRP5,DEL_BLOCK,DOC_CATEG,SALES_OFF,SALES_GRP,SALESORG,DISTR_CHAN,REASON_REJ,CH_ON,ORDER_PROB,GROSS_WGT,BWAPPLNM,PROCESSKEY,BATCH,EXCHG_CRD,EANUPC,CREATEDON,CREATEDBY,CREA_TIME,BILBLK_ITM,UNIT_OF_WT,CML_CF_QTY,CML_CD_QTY,COND_UNIT,SALESDEAL,COND_PR_UN,CML_OR_QTY,SUBTOTAL_1,SUBTOTAL_2,SUBTOTAL_3,SUBTOTAL_4,SUBTOTAL_5,SUBTOTAL_6,MIN_DL_QTY,STOR_LOC,REQDEL_QTY,MATL_GROUP,MATERIAL,MAT_ENTRD,BASE_UOM,MATL_GRP_1,MATL_GRP_2,MATL_GRP_3,MATL_GRP_4,MATL_GRP_5,TAX_VALUE,NET_PRICE,NET_VALUE,NET_WT_AP,UNLD_PT_WE,BILLTOPRTY,PAYER,SHIP_TO,PROD_HIER,FORWAGENT,ITEM_CATEG,SALESEMPLY,ROUTE,SHIP_STCK,DIVISION,STAT_DATE,EXCHG_STAT,SUB_REASON,BND_I...bofCurrentPath12: String = /data/dataset/enrichment/txn_proc/bof_o12/current/\nzarixsd2Path: String = /data/dataset/raw/txn_proc/zarixsd2/current/\nz2DF: org.apache.spark.sql.DataFrame = [COL_NAME: string, REQUEST: string ... 16 more fields]\nzarixsd2: org.apache.spark.sql.DataFrame = [DOC_NUMBER: string, S_ORD_ITEM: string ... 1 more field]\nbofGenericFieldLists: String => (List[String], List[String], List[String], List[String], List[String], List[String]) = <function1>\nbofFieldLists: (List[String], List[String], List[String], List[String], List[String], List[String]) = (List(DOC_NUMBER, S_ORD_ITEM),List(\"\"),List(\"\"),List(\"\"),List(\"\"),List(DOC_NUMBER, S_ORD_ITEM, STS_ITM, STS_BILL, STS_PRC, STS_DEL, DOC_TYPE, COMP_CODE, LOC_CURRCY, SOLD_TO, CUST_GRP1, CUST_GRP2, CUST_GRP3, CUST_GRP4, CUST_GRP5, DEL_BLOCK, DOC_CATEG, SALES_OFF, SALES_GRP, SALESORG, DISTR_CHAN, REASON_REJ, CH_ON, ORDER_PROB, GROSS_WGT, BWAPPLNM, PROCESSKEY, BATCH, EXCHG_CRD, EANUPC, CREATEDON, CREATEDBY, CREA_TIME, BILBLK_ITM, UNIT_OF_WT, CML_CF_QTY, CML_CD_QTY, COND_UNIT, SALESDEAL, COND_PR_UN, CML_OR_QTY, SUBTOTAL_1, SUBTOTAL_2, SUBTOTAL_3, SUBTOTAL_4, SUBTOTAL_5, SUBTOTAL_6, MIN_DL_QTY, STOR_LOC, REQDEL_QTY, MATL_GROUP, MATERIAL, MAT_ENTRD, BASE_UOM, MATL_GRP_1, MATL_GRP_2, MATL_GRP_3...bofSchema: org.apache.spark.sql.types.StructType = StructType(StructField(DOC_NUMBER,StringType,true), StructField(S_ORD_ITEM,StringType,true), StructField(STS_ITM,StringType,false), StructField(STS_BILL,StringType,false), StructField(STS_PRC,StringType,false), StructField(STS_DEL,StringType,false), StructField(DOC_TYPE,StringType,false), StructField(COMP_CODE,StringType,false), StructField(LOC_CURRCY,StringType,false), StructField(SOLD_TO,StringType,false), StructField(CUST_GRP1,StringType,false), StructField(CUST_GRP2,StringType,false), StructField(CUST_GRP3,StringType,false), StructField(CUST_GRP4,StringType,false), StructField(CUST_GRP5,StringType,false), StructField(DEL_BLOCK,StringType,false), StructField(DOC_CATEG,StringType,false), StructField(SALES_OFF,StringType,false), Struct...bofo12: org.apache.spark.sql.DataFrame = [DOC_NUMBER: string, S_ORD_ITEM: string ... 47 more fields]\nbofOrderedFieldsdfIPG: String = BCNFGIPG,OBJVERS,CHANGED,BPRICODE,BCNFCHR01\nbofCurrentPathdfIPG: String = /data/dataset/enrichment/txn_proc/bcnfgipg/current/\nbofFieldListsdfIPG: (List[String], List[String], List[String], List[String], List[String], List[String]) = (List(DOC_NUMBER, S_ORD_ITEM),List(\"\"),List(\"\"),List(\"\"),List(\"\"),List(BCNFGIPG, OBJVERS, CHANGED, BPRICODE, BCNFCHR01))\nbofSchema: org.apache.spark.sql.types.StructType = StructType(StructField(BCNFGIPG,StringType,false), StructField(OBJVERS,StringType,false), StructField(CHANGED,StringType,false), StructField(BPRICODE,StringType,false), StructField(BCNFCHR01,StringType,false))\nbofoIPG: org.apache.spark.sql.DataFrame = [BCNFGIPG: string, OBJVERS: string ... 3 more fields]\nbofOrderedFieldsdfMPC: String = BCNFGBMPC,OBJVERS,CHANGED,BACQADVCD,MATL_GROUP,BBAGITMID,BDEPOTFLG,BFRGNMLTY,BCNFCHR15\nbofCurrentPathdfMPC: String = /data/dataset/enrichment/txn_proc/bcnfgbmpc/current/\nbofFieldListsdfMPC: (List[String], List[String], List[String], List[String], List[String], List[String]) = (List(DOC_NUMBER, S_ORD_ITEM),List(\"\"),List(\"\"),List(\"\"),List(\"\"),List(BCNFGBMPC, OBJVERS, CHANGED, BACQADVCD, MATL_GROUP, BBAGITMID, BDEPOTFLG, BFRGNMLTY, BCNFCHR15))\nbofSchema: org.apache.spark.sql.types.StructType = StructType(StructField(BCNFGBMPC,StringType,false), StructField(OBJVERS,StringType,false), StructField(CHANGED,StringType,false), StructField(BACQADVCD,StringType,false), StructField(MATL_GROUP,StringType,false), StructField(BBAGITMID,StringType,false), StructField(BDEPOTFLG,StringType,false), StructField(BFRGNMLTY,StringType,false), StructField(BCNFCHR15,StringType,false))\nbofoMPC: org.apache.spark.sql.DataFrame = [BCNFGBMPC: string, OBJVERS: string ... 7 more fields]\nbofOrderedFieldsdfORDT: String = BCNFGORDT,RSOBJVERS,RSRCHAGEFLAG,BMATCATGP,ITEM_CATEG,BCNFCHR01\nbofCurrentPathdfORDT: String = /data/dataset/enrichment/txn_proc/bcnfgordt/current/\nbofFieldListsdfORDT: (List[String], List[String], List[String], List[String], List[String], List[String]) = (List(DOC_NUMBER, S_ORD_ITEM),List(\"\"),List(\"\"),List(\"\"),List(\"\"),List(BCNFGORDT, RSOBJVERS, RSRCHAGEFLAG, BMATCATGP, ITEM_CATEG, BCNFCHR01))\nbofSchema: org.apache.spark.sql.types.StructType = StructType(StructField(BCNFGORDT,StringType,false), StructField(RSOBJVERS,StringType,false), StructField(RSRCHAGEFLAG,StringType,false), StructField(BMATCATGP,StringType,false), StructField(ITEM_CATEG,StringType,false), StructField(BCNFCHR01,StringType,false))\nbofoORDT: org.apache.spark.sql.DataFrame = [BCNFGORDT: string, RSOBJVERS: string ... 4 more fields]\nbofOrderedFieldsdfMM: String = MATERIAL,OBJVERS,CHANGED,AF_COLOR,AF_FCOCO,AF_GENDER,AF_GRID,AF_STYLE,APO_PROD,BASE_UOM,BASIC_MATL,BBP_PROD,COMPETITOR,CONT_UNIT,CREATEDON,CRM_PROD,DIVISION,EANUPC,EXTMATLGRP,GROSS_CONT,GROSS_WT,HC_AGENT1,HC_AGENT2,HC_AGENT3,HC_ANESIND,HC_APPRTYP,HC_ATCCODE,HC_ATCMTYP,HC_CATIND1,HC_CATIND2,HC_CATIND3,HC_HAZMIND,HC_IMPMIND,IND_SECTOR,LOGSYS,MANUFACTOR,MANU_MATNR,MATL_CAT,MATL_GROUP,MATL_TYPE,MSA_USAGE,NET_CONT,NET_WEIGHT,PO_UNIT,PROD_HIER,RT_CONFMAT,RT_PRBAND,RT_PRRULE,RT_SEASON,RT_SEAYR,RT_SUPS,SIZE_DIM,STD_DESCR,UCCERTIFTY,UCCONSTCLA,UCFUNCCLAS,UNIT_DIM,UNIT_OF_WT,VOLUME,VOLUMEUNIT,BACQMTDCD,BACQMTDSX,BACQADVCD,BRTNMTDCD,BITMCNSRT,BSVCUCARM,BITMSTDCD,BITMNAMCD,BPRCMTLCD,BDEMILCD,BTECHCONT,BQUALCONT,BPACKCONT,BDEMNDPLR,BSUPLYPLR,BADPCODE,BENVATRCD,BHZDMATC...bofCurrentPathdfMM: String = /data/dataset/enrichment/txn_proc/0material/current/\nbofFieldListsdfMM: (List[String], List[String], List[String], List[String], List[String], List[String]) = (List(DOC_NUMBER, S_ORD_ITEM),List(\"\"),List(\"\"),List(\"\"),List(\"\"),List(MATERIAL, OBJVERS, CHANGED, AF_COLOR, AF_FCOCO, AF_GENDER, AF_GRID, AF_STYLE, APO_PROD, BASE_UOM, BASIC_MATL, BBP_PROD, COMPETITOR, CONT_UNIT, CREATEDON, CRM_PROD, DIVISION, EANUPC, EXTMATLGRP, GROSS_CONT, GROSS_WT, HC_AGENT1, HC_AGENT2, HC_AGENT3, HC_ANESIND, HC_APPRTYP, HC_ATCCODE, HC_ATCMTYP, HC_CATIND1, HC_CATIND2, HC_CATIND3, HC_HAZMIND, HC_IMPMIND, IND_SECTOR, LOGSYS, MANUFACTOR, MANU_MATNR, MATL_CAT, MATL_GROUP, MATL_TYPE, MSA_USAGE, NET_CONT, NET_WEIGHT, PO_UNIT, PROD_HIER, RT_CONFMAT, RT_PRBAND, RT_PRRULE, RT_SEASON, RT_SEAYR, RT_SUPS, SIZE_DIM, STD_DESCR, UCCERTIFTY, UCCONSTCLA, UCFUNCCLAS, UNIT_DIM, UN...bofSchema: org.apache.spark.sql.types.StructType = StructType(StructField(MATERIAL,StringType,false), StructField(OBJVERS,StringType,false), StructField(CHANGED,StringType,false), StructField(AF_COLOR,StringType,false), StructField(AF_FCOCO,StringType,false), StructField(AF_GENDER,StringType,false), StructField(AF_GRID,StringType,false), StructField(AF_STYLE,StringType,false), StructField(APO_PROD,StringType,false), StructField(BASE_UOM,StringType,false), StructField(BASIC_MATL,StringType,false), StructField(BBP_PROD,StringType,false), StructField(COMPETITOR,StringType,false), StructField(CONT_UNIT,StringType,false), StructField(CREATEDON,StringType,false), StructField(CRM_PROD,StringType,false), StructField(DIVISION,StringType,false), StructField(EANUPC,StringType,false), StructField(E...bofoMM: org.apache.spark.sql.DataFrame = [MATERIAL: string, OBJVERS: string ... 266 more fields]\nbofOrderedFieldsdfDEPMAIN: String = BDEPMAIN,OBJVERS,CHANGED\nbofCurrentPathdfDEPMAIN: String = /data/dataset/enrichment/txn_proc/bdepmain/current/\nbofFieldListsdfDEPMAIN: (List[String], List[String], List[String], List[String], List[String], List[String]) = (List(DOC_NUMBER, S_ORD_ITEM),List(\"\"),List(\"\"),List(\"\"),List(\"\"),List(BDEPMAIN, OBJVERS, CHANGED))\nbofSchema: org.apache.spark.sql.types.StructType = StructType(StructField(BDEPMAIN,StringType,false), StructField(OBJVERS,StringType,false), StructField(CHANGED,StringType,false))\ndfDEPMAIN: org.apache.spark.sql.DataFrame = [BDEPMAIN: string, OBJVERS: string ... 1 more field]\nbofOrderedFields13: String = DOC_NUMBER,S_ORD_ITEM,SALES_UNIT,RECORDMODE,BTKILLUOM,BTKILLQTY,BTVRSTCD,BTSVCCD,BTLVLAPKG,BTCONTPO,BTCCTRLNO,BTBUNDLES,BTPRCSIZE,BTRCSTCTR,BTCRID,BTSECONCD,BTCSHPLOC,BTORDTYPE,BTORDQTY,BTCORLSTN,BTCUSTCNO,BTPVACTNO,BTTRANSIN,BTTREATED,BTPRODID,BTWEAPSYS,BTPVPNO,BTPRCLIST,BTPVPRDID,BTPRCIND,BTBINLOC,BTIMESTMP,BCNT_NBR,BTVRAPDT,BEARCDTE\nbofCurrentPath13: String = /data/dataset/enrichment/txn_proc/bof_o13/current/\nbofFieldLists: (List[String], List[String], List[String], List[String], List[String], List[String]) = (List(DOC_NUMBER, S_ORD_ITEM),List(\"\"),List(\"\"),List(\"\"),List(\"\"),List(DOC_NUMBER, S_ORD_ITEM, SALES_UNIT, RECORDMODE, BTKILLUOM, BTKILLQTY, BTVRSTCD, BTSVCCD, BTLVLAPKG, BTCONTPO, BTCCTRLNO, BTBUNDLES, BTPRCSIZE, BTRCSTCTR, BTCRID, BTSECONCD, BTCSHPLOC, BTORDTYPE, BTORDQTY, BTCORLSTN, BTCUSTCNO, BTPVACTNO, BTTRANSIN, BTTREATED, BTPRODID, BTWEAPSYS, BTPVPNO, BTPRCLIST, BTPVPRDID, BTPRCIND, BTBINLOC, BTIMESTMP, BCNT_NBR, BTVRAPDT, BEARCDTE))\nbofSchema: org.apache.spark.sql.types.StructType = StructType(StructField(DOC_NUMBER,StringType,true), StructField(S_ORD_ITEM,StringType,true), StructField(SALES_UNIT,StringType,false), StructField(RECORDMODE,StringType,false), StructField(BTKILLUOM,StringType,false), StructField(BTKILLQTY,StringType,false), StructField(BTVRSTCD,StringType,false), StructField(BTSVCCD,StringType,false), StructField(BTLVLAPKG,StringType,false), StructField(BTCONTPO,StringType,false), StructField(BTCCTRLNO,StringType,false), StructField(BTBUNDLES,StringType,false), StructField(BTPRCSIZE,StringType,false), StructField(BTRCSTCTR,StringType,false), StructField(BTCRID,StringType,false), StructField(BTSECONCD,StringType,false), StructField(BTCSHPLOC,StringType,false), StructField(BTORDTYPE,StringType,false), St...bofo13: org.apache.spark.sql.DataFrame = [DOC_NUMBER: string, S_ORD_ITEM: string ... 5 more fields]\nbofOrderedFields14: String = DOC_NUMBER,S_ORD_ITEM,SCHED_LINE,STORNO,REJECTN_ST,STS_ITM,STS_BILL,STS_PRC,STS_DEL,CONF_QTY,CORR_QTY,REQ_DATE,SCHD_CATEG,LOAD_DATE,DLV_BLOCK,REQ_QTY,MATAV_DATE,BASE_UOM,TRNSD_DATE,SALES_UNIT,GI_DATE,DSDEL_DATE,ORDER_QTY,BBILL_AD,BBO_TYPE,BBILL_ST,BCNDQTY,BDMDQTY,UNIT,BMATRCVDT,BSUFX_CD,BSUPP_ST,QUOT_FROM,DOC_TYPE,ORD_REASON,QUOT_TO,COMP_CODE,BILL_BLOCK,LOC_CURRCY,SOLD_TO,RATE_TYPE,CUST_GRP1,CUST_GRP2,CUST_GRP3,CUST_GRP4,CUST_GRP5,DEL_BLOCK,STAT_CURR,DOC_CATEG,SALES_OFF,SALES_GRP,SALESORG,DISTR_CHAN,REASON_REJ,CH_ON,ORDER_PROB,BATCH,EXCHG_CRD,EANUPC,CREATEDON,CREATEDBY,CREA_TIME,BILBLK_ITM,UNIT_OF_WT,COND_UNIT,COND_PR_UN,STOR_LOC,MATL_GROUP,MATERIAL,MAT_ENTRD,MATL_GRP_1,MATL_GRP_2,MATL_GRP_3,MATL_GRP_4,MATL_GRP_5,NET_PRICE,UNLD_PT_WE,BILLTOPRTY,PAYER,SHIP_TO...bofCurrentPath14: String = /data/dataset/enrichment/txn_proc/bof_o14/current/\nbofFieldLists: (List[String], List[String], List[String], List[String], List[String], List[String]) = (List(DOC_NUMBER, S_ORD_ITEM),List(\"\"),List(\"\"),List(\"\"),List(\"\"),List(DOC_NUMBER, S_ORD_ITEM, SCHED_LINE, STORNO, REJECTN_ST, STS_ITM, STS_BILL, STS_PRC, STS_DEL, CONF_QTY, CORR_QTY, REQ_DATE, SCHD_CATEG, LOAD_DATE, DLV_BLOCK, REQ_QTY, MATAV_DATE, BASE_UOM, TRNSD_DATE, SALES_UNIT, GI_DATE, DSDEL_DATE, ORDER_QTY, BBILL_AD, BBO_TYPE, BBILL_ST, BCNDQTY, BDMDQTY, UNIT, BMATRCVDT, BSUFX_CD, BSUPP_ST, QUOT_FROM, DOC_TYPE, ORD_REASON, QUOT_TO, COMP_CODE, BILL_BLOCK, LOC_CURRCY, SOLD_TO, RATE_TYPE, CUST_GRP1, CUST_GRP2, CUST_GRP3, CUST_GRP4, CUST_GRP5, DEL_BLOCK, STAT_CURR, DOC_CATEG, SALES_OFF, SALES_GRP, SALESORG, DISTR_CHAN, REASON_REJ, CH_ON, ORDER_PROB, BATCH, EXCHG_CRD, EANUPC, CREATEDON...bofSchema: org.apache.spark.sql.types.StructType = StructType(StructField(DOC_NUMBER,StringType,true), StructField(S_ORD_ITEM,StringType,true), StructField(SCHED_LINE,StringType,false), StructField(STORNO,StringType,false), StructField(REJECTN_ST,StringType,false), StructField(STS_ITM,StringType,false), StructField(STS_BILL,StringType,false), StructField(STS_PRC,StringType,false), StructField(STS_DEL,StringType,false), StructField(CONF_QTY,StringType,false), StructField(CORR_QTY,StringType,false), StructField(REQ_DATE,StringType,false), StructField(SCHD_CATEG,StringType,false), StructField(LOAD_DATE,StringType,false), StructField(DLV_BLOCK,StringType,false), StructField(REQ_QTY,StringType,false), StructField(MATAV_DATE,StringType,false), StructField(BASE_UOM,StringType,false), StructFie...bofo14: org.apache.spark.sql.DataFrame = [DOC_NUMBER: string, S_ORD_ITEM: string ... 10 more fields]\nbofOrderedFields15: String = DOC_NUMBER,S_ORD_ITEM,SCHED_LINE,DELIV_NUMB,DELIV_ITEM,REQU_QTY,CONF_QTY,DLV_QTY,SALES_UNIT,DLV_STSO,DLV_STS,GI_STS,LW_GISTS,DSDEL_DATE,DSDEL_TIME,CONF_DATE,CONF_TIME,PLD_GI_DTE,ACT_GI_DTE,LST_A_GD,ACT_DL_DTE,ACT_DL_TME,LST_A_DD,LST_A_DT,DLV_BLOCKD,BASE_UOM,DENOMINTR,NUMERATOR,DOC_CATEG,SCHED_DEL,REC_DELETD,BMATL_BLK,DLVQEYCR,DLVQEYSC,DLVQLECR,DLVQLESC,BCOUNTER,RECORDMODE,BCNDQTY,BEARCDTE\nbofCurrentPath15: String = /data/dataset/enrichment/txn_proc/bof_o15/current/\nbofFieldLists: (List[String], List[String], List[String], List[String], List[String], List[String]) = (List(DOC_NUMBER, S_ORD_ITEM),List(\"\"),List(\"\"),List(\"\"),List(\"\"),List(DOC_NUMBER, S_ORD_ITEM, SCHED_LINE, DELIV_NUMB, DELIV_ITEM, REQU_QTY, CONF_QTY, DLV_QTY, SALES_UNIT, DLV_STSO, DLV_STS, GI_STS, LW_GISTS, DSDEL_DATE, DSDEL_TIME, CONF_DATE, CONF_TIME, PLD_GI_DTE, ACT_GI_DTE, LST_A_GD, ACT_DL_DTE, ACT_DL_TME, LST_A_DD, LST_A_DT, DLV_BLOCKD, BASE_UOM, DENOMINTR, NUMERATOR, DOC_CATEG, SCHED_DEL, REC_DELETD, BMATL_BLK, DLVQEYCR, DLVQEYSC, DLVQLECR, DLVQLESC, BCOUNTER, RECORDMODE, BCNDQTY, BEARCDTE))\nbofSchema: org.apache.spark.sql.types.StructType = StructType(StructField(DOC_NUMBER,StringType,true), StructField(S_ORD_ITEM,StringType,true), StructField(SCHED_LINE,StringType,false), StructField(DELIV_NUMB,StringType,false), StructField(DELIV_ITEM,StringType,false), StructField(REQU_QTY,StringType,false), StructField(CONF_QTY,StringType,false), StructField(DLV_QTY,StringType,false), StructField(SALES_UNIT,StringType,false), StructField(DLV_STSO,StringType,false), StructField(DLV_STS,StringType,false), StructField(GI_STS,StringType,false), StructField(LW_GISTS,StringType,false), StructField(DSDEL_DATE,StringType,false), StructField(DSDEL_TIME,StringType,false), StructField(CONF_DATE,StringType,false), StructField(CONF_TIME,StringType,false), StructField(PLD_GI_DTE,StringType,false), S...bofo15: org.apache.spark.sql.DataFrame = [DOC_NUMBER: string, S_ORD_ITEM: string ... 11 more fields]\nbofOrderedFields17: String = DELIV_NUMB,DELIV_ITEM,REFER_DOC,REFER_ITM,DLV_QTY,SALES_UNIT,CREATEDON,SHIP_DATE,DEL_TYPE,GI_DATE,ITEM_CATEG,ITM_TYPE,MATERIAL,MATL_GROUP,PLANT,DOC_CATEG,SHIP_TO,SOLD_TO,ACT_DL_QTY,BASE_UOM,RECORDMODE,ACT_GI_DTE,LOAD_DATE,BILLTOPRTY,CH_ON,CREA_TIME,PROCESSKEY,GOODSMV_ST,DLV_STSOI,BSHIPTYP,STORNO,PRVDOC_CTG,STOR_LOC,BEARCDTE\nbofCurrentPath17: String = /data/dataset/enrichment/txn_proc/bof_o17/current/\nbofFieldLists: (List[String], List[String], List[String], List[String], List[String], List[String]) = (List(DOC_NUMBER, S_ORD_ITEM),List(\"\"),List(\"\"),List(\"\"),List(\"\"),List(DELIV_NUMB, DELIV_ITEM, REFER_DOC, REFER_ITM, DLV_QTY, SALES_UNIT, CREATEDON, SHIP_DATE, DEL_TYPE, GI_DATE, ITEM_CATEG, ITM_TYPE, MATERIAL, MATL_GROUP, PLANT, DOC_CATEG, SHIP_TO, SOLD_TO, ACT_DL_QTY, BASE_UOM, RECORDMODE, ACT_GI_DTE, LOAD_DATE, BILLTOPRTY, CH_ON, CREA_TIME, PROCESSKEY, GOODSMV_ST, DLV_STSOI, BSHIPTYP, STORNO, PRVDOC_CTG, STOR_LOC, BEARCDTE))\nbofSchema: org.apache.spark.sql.types.StructType = StructType(StructField(DELIV_NUMB,StringType,false), StructField(DELIV_ITEM,StringType,false), StructField(REFER_DOC,StringType,false), StructField(REFER_ITM,StringType,false), StructField(DLV_QTY,StringType,false), StructField(SALES_UNIT,StringType,false), StructField(CREATEDON,StringType,false), StructField(SHIP_DATE,StringType,false), StructField(DEL_TYPE,StringType,false), StructField(GI_DATE,StringType,false), StructField(ITEM_CATEG,StringType,false), StructField(ITM_TYPE,StringType,false), StructField(MATERIAL,StringType,false), StructField(MATL_GROUP,StringType,false), StructField(PLANT,StringType,false), StructField(DOC_CATEG,StringType,false), StructField(SHIP_TO,StringType,false), StructField(SOLD_TO,StringType,false), StructF...bofo17: org.apache.spark.sql.DataFrame = [DELIV_NUMB: string, DELIV_ITEM: string ... 32 more fields]\nbofOrderedFields20: String = DOC_NUMBER,S_ORD_ITEM,BFMS_PRG,BFMSCSCD,BFMS_IND,BMILSVC_1,BMILSVC_2,BMILSVC_G,BMILSVC_S,BORDER_DT,MIN_DL_QTY,BASE_UOM,LOWR_BND,DOC_CURRCY,BPURPRIC,RTPLCST,CML_CF_QTY,BTORDQTY,BTKILLQTY,BTKILLUOM,BTVRSTCD,CREATEDON,CONF_QTY,NET_VALUE,REQ_QTY,BCNDQTY,BDMDQTY,GIS_QTY,BCONVERT,MATERIAL,BIPG,SUBTOTAL_3,SUBTOTAL_4,SUBTOTAL_5,SUBTOTAL_6,SUBTOTAL_2,SUBTOTAL_1,QCOASREQ,LOAD_DATE,SOLD_TO,DLV_QTY,REQDEL_QTY,BGR_DATE,RECORDMODE,BCUSTPODT,BNS_RDD,BORD_TYPE,BBSTNK,BORGDNUM,BORGNLDOC,BPRI_CD,CML_OR_QTY,DSDEL_DATE,BMPC,ITEM_CATEG,REASON_REJ,LOC_CURRCY,UPPR_BND,SALES_UNIT,GR_QTY,BSHIPTYP,BITMCATGP,BTSALEUNT,COND_PR_UN,NET_PRICE,BPONBR,ACT_GI_DTE,DOC_TYPE,PO_UNIT,UNIT,PCONF_QTY,CML_CD_QTY,CONF_DATE,BCAGECDPN,BIDNLF,BMFRPN,DLVQEYCR,DLVQEYSC,DLVQLECR,BMATRCVDT,BNMCS_IND,BCASRE...bofCurrentPath20: String = /data/dataset/enrichment/txn_proc/bof_o20/current/\nbofFieldLists: (List[String], List[String], List[String], List[String], List[String], List[String]) = (List(DOC_NUMBER, S_ORD_ITEM),List(\"\"),List(\"\"),List(\"\"),List(\"\"),List(DOC_NUMBER, S_ORD_ITEM, BFMS_PRG, BFMSCSCD, BFMS_IND, BMILSVC_1, BMILSVC_2, BMILSVC_G, BMILSVC_S, BORDER_DT, MIN_DL_QTY, BASE_UOM, LOWR_BND, DOC_CURRCY, BPURPRIC, RTPLCST, CML_CF_QTY, BTORDQTY, BTKILLQTY, BTKILLUOM, BTVRSTCD, CREATEDON, CONF_QTY, NET_VALUE, REQ_QTY, BCNDQTY, BDMDQTY, GIS_QTY, BCONVERT, MATERIAL, BIPG, SUBTOTAL_3, SUBTOTAL_4, SUBTOTAL_5, SUBTOTAL_6, SUBTOTAL_2, SUBTOTAL_1, QCOASREQ, LOAD_DATE, SOLD_TO, DLV_QTY, REQDEL_QTY, BGR_DATE, RECORDMODE, BCUSTPODT, BNS_RDD, BORD_TYPE, BBSTNK, BORGDNUM, BORGNLDOC, BPRI_CD, CML_OR_QTY, DSDEL_DATE, BMPC, ITEM_CATEG, REASON_REJ, LOC_CURRCY, UPPR_BND, SALES_UNIT, GR...bofSchema: org.apache.spark.sql.types.StructType = StructType(StructField(DOC_NUMBER,StringType,true), StructField(S_ORD_ITEM,StringType,true), StructField(BFMS_PRG,StringType,false), StructField(BFMSCSCD,StringType,false), StructField(BFMS_IND,StringType,false), StructField(BMILSVC_1,StringType,false), StructField(BMILSVC_2,StringType,false), StructField(BMILSVC_G,StringType,false), StructField(BMILSVC_S,StringType,false), StructField(BORDER_DT,StringType,false), StructField(MIN_DL_QTY,StringType,false), StructField(BASE_UOM,StringType,false), StructField(LOWR_BND,StringType,false), StructField(DOC_CURRCY,StringType,false), StructField(BPURPRIC,StringType,false), StructField(RTPLCST,StringType,false), StructField(CML_CF_QTY,StringType,false), StructField(BTORDQTY,StringType,false), Str...bofo20: org.apache.spark.sql.DataFrame = [DOC_NUMBER: string, S_ORD_ITEM: string ... 97 more fields]\nbofo21: org.apache.spark.sql.DataFrame = [DOC_NUMBER: string, S_ORD_ITEM: string ... 14 more fields]\n"}]},"apps":[],"jobName":"paragraph_1512659779236_-1695556431","id":"20171020-030354_1391705457","dateCreated":"2017-12-07T10:16:19-0500","dateStarted":"2017-12-19T17:36:56-0500","dateFinished":"2017-12-19T17:37:53-0500","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:14802"},{"text":"%spark2\r\ngetTimeDifference(bofo20Start)\r\n\r\nval bofo12Two = bofo12.filter($\"DOC_CATEG\" === \"C\" || $\"DOC_CATEG\" === \"I\")\r\nval dfIPGTwo = bofoIPG.filter($\"BCNFGIPG\" =!= \"\")\r\nval dfMPCTwo =  bofoMPC.filter($\"BCNFGBMPC\" =!= \"\").withColumn(\"BCNFGBMPC\", $\"BCNFGBMPC\".cast(\"int\"))\r\nval dfORDTTwo = bofoORDT.filter($\"BCNFGORDT\" =!= \"\").select($\"ITEM_CATEG\", $\"BMATCATGP\", $\"BCNFCHR01\")\r\nval uniqueMatl = bofo12Two.select($\"MATERIAL\", $\"BDODAAC\").dropDuplicates(\"MATERIAL\")\r\nuniqueMatl.persist()\r\n// note that these fields are also in bofo12\r\nval dfMaterial = bofoMM.join(uniqueMatl, Seq(\"MATERIAL\"), \"inner\").where($\"OBJVERS\" === \"A\")\r\n.selectExpr(\"MATERIAL\", \"BDODAAC\", \"BACQADVCD as BACQADVCDTwo\", \"MATL_GRP_4 AS MATL_GRP_4Two\", \"BBAGITMID AS BBAGITMIDTwo\", \"BMANU_UOM AS BMANU_UOMTwo\", \"BASE_UOM AS BASE_UOMTwo\", \"BITMCATGP AS BITMCATGPTwo\")\r\n\r\ndef setBITMCATGP(dfMatl: DataFrame)(df: DataFrame): DataFrame = {\r\n// set bitmcatgp to dfMaterial's bitmcatgp if material matches\r\n  df.join(dfMatl.selectExpr(\"MATERIAL\", \"BITMCATGPTwo as dfMatlBITMCATGPTwo\"), Seq(\"MATERIAL\"), \"left\")\r\n  .withColumn(\"BITMCATGP\", coalesce($\"dfMatlBITMCATGPTwo\", $\"BITMCATGP\"))\r\n  .drop(\"dfMatlBITMCATGPTwo\")\r\n}\r\n\r\n\r\ndef deriveBFMSCSCD(df: DataFrame): DataFrame = {\r\n  // the < 5 might be pointless since we want to take a substring from idx 4-6 anyways.\r\n  df.withColumn(\"BFMSCSCD\", when(length($\"BSUPPADR\") < 3 || substring($\"BSUPPADR\", 4, 1) === \"!\" || (substring($\"BSUPPADR\", 4, 1) === \"#\" && length($\"BSUPPADR\") < 5), \"\").otherwise(substring($\"BSUPPADR\", 4, 3)))\r\n}\r\n\r\n\r\ndef deriveBCONVERT(df: DataFrame): DataFrame = {\r\n  df.withColumn(\"BCONVERT\", when(upper($\"BBSTNK\") === \"ORIGINAL\" or upper($\"BBSTNK\") === \"CONVERSION,BB\", \"Y\").otherwise(\"N\"))\r\n}\r\n\r\n\r\n// left join, 3 columns, address null, assume i_bcnfgipg has unique key\r\n// check i_bcnfgipg if there are empty values, if not can assume empty bpri_code will return \"\"\r\n// else add a check for dfIPG\r\n\r\n// <=> for join will match nulls with nulls. See if needed down the line, most likely not. === ignores nulls when matching.\r\n// check in general if itabs have nulls\r\n\r\ndef deriveBIPG(dfiIPG: DataFrame)(df: DataFrame): DataFrame = {\r\n  df.join(dfiIPG.select($\"BPRICODE\", $\"BCNFCHR01\"), $\"BPRI_CD\" === $\"BPRICODE\", \"left\").withColumnRenamed(\"BCNFCHR01\", \"BIPG\")\r\n}\r\n\r\n\r\ndef deriveBORD_TYPE(dfiORDT: DataFrame)(df: DataFrame): DataFrame = {\r\n\r\n// replace empty values of BOFITMCAT w/ \"ZDUL\"\r\n  val dfTwo = df.withColumn(\"v_BITMCATGP\", when($\"BOFITMCAT\" === \"\", \"ZDUL\").otherwise($\"BOFITMCAT\"))\r\n\r\n// 3 posibilities\r\n  def categoryCheck(df: DataFrame, dfiORDT: DataFrame): DataFrame = {\r\n  \tval categoryCheckOne = df.select($\"DOC_NUMBER\", $\"S_ORD_ITEM\", $\"ITEM_CATEG\")\r\n    .join(dfiORDT.filter($\"BMATCATGP\" === \"*\"), df.col(\"ITEM_CATEG\") === dfiORDT.col(\"ITEM_CATEG\"), \"left\")\r\n  \t.withColumnRenamed(\"BCNFCHR01\", \"b1\")\r\n\r\n  \tval categoryCheckTwo = df.select($\"DOC_NUMBER\", $\"S_ORD_ITEM\", $\"v_BITMCATGP\")\r\n    .join(dfiORDT.filter($\"ITEM_CATEG\" === \"*\"), df.col(\"v_BITMCATGP\") === dfiORDT.col(\"BMATCATGP\"), \"left\")\r\n  \t.withColumnRenamed(\"BCNFCHR01\", \"b2\")\r\n\r\n\r\n  \tval categoryCheckThree = df.select($\"DOC_NUMBER\", $\"S_ORD_ITEM\", $\"v_BITMCATGP\", $\"ITEM_CATEG\")\r\n    .join(dfiORDT, df.col(\"ITEM_CATEG\") === dfiORDT.col(\"ITEM_CATEG\") && df.col(\"v_BITMCATGP\") === dfiORDT.col(\"BMATCATGP\") , \"left\")\r\n  \t.withColumnRenamed(\"BCNFCHR01\", \"b3\")\r\n\r\n  \tcategoryCheckOne.join(categoryCheckTwo, Seq(\"DOC_NUMBER\", \"S_ORD_ITEM\"), \"inner\")\r\n  \t.join(categoryCheckThree, Seq(\"DOC_NUMBER\", \"S_ORD_ITEM\"), \"inner\")\r\n  \t.withColumn(\"BORD_TYPE\", coalesce($\"b1\", $\"b2\", $\"b3\"))\r\n  \t.select(\"DOC_NUMBER\", \"S_ORD_ITEM\", \"BORD_TYPE\")\r\n  }\r\n\r\n  // also check if item category is not empty\r\n  dfTwo.join(categoryCheck(dfTwo, dfiORDT), Seq(\"DOC_NUMBER\", \"S_ORD_ITEM\"), \"inner\")\r\n  .withColumn(\"BORD_TYPE\", when($\"ITEM_CATEG\".isNotNull, $\"BORD_TYPE\").otherwise(\"\")) \r\n}\r\n\r\n\r\n\r\n\r\n// Derive BMPC\r\n\r\nval windowSpecMatlRule = Window.partitionBy(\"MATERIAL\").orderBy(\"BCNFGBMPC\")\r\n// insert DODAAC from above, not from dfMaterial\r\ndef deriveBMPC(dfMatlCheck: DataFrame, dfiMPC: DataFrame, dfiDEPMAIN: DataFrame)(df: DataFrame): DataFrame = {\r\nval dfMatlRule = dfMatlCheck.join(dfiMPC, (dfiMPC.col(\"BACQADVCD\") === \"*\" || dfiMPC.col(\"BACQADVCD\") === dfMatlCheck.col(\"BACQADVCDTwo\"))\r\n&& (dfiMPC.col(\"MATL_GROUP\") === \"*\" || dfiMPC.col(\"MATL_GROUP\") === dfMatlCheck.col(\"MATL_GRP_4Two\"))\r\n&& (dfiMPC.col(\"BBAGITMID\") === \"*\" || dfiMPC.col(\"BBAGITMID\") === dfMatlCheck.col(\"BBAGITMIDTwo\"))\r\n&& (dfiMPC.col(\"BFRGNMLTY\") === \"*\" || dfiMPC.col(\"BFRGNMLTY\") === dfMatlCheck.col(\"BDODAAC\")), \"inner\") // these 4 categories should correspond to unique MATERIALs as defined. see if groupby works\r\n// check bdodaac vs bdepmain from depmain table, namely if flag is Y and they match, or flag is N and they indeed do not match\r\n.join(dfiDEPMAIN.select(\"BDEPMAIN\"), $\"BDODAAC\" === $\"BDEPMAIN\", \"left\")\r\n.filter($\"BDODAAC\" === \"*\" || ($\"BDEPOTFLG\" === \"Y\" && $\"BDODAAC\".isNotNull) || ($\"BDEPOTFLG\" === \"N\" && $\"BDODAAC\".isNull))\r\n.withColumn(\"RuleToUse\", rank().over(windowSpecMatlRule)).where($\"RuleToUse\" === 1)\r\n/* alternative\r\n\r\nval dfMatlRule = dfMatlCheck.crossJoin(dfiMPC).where((dfiMPC.col(\"BACQADVCD\") === \"*\" || dfiMPC.col(\"BACQADVCD\") === dfMatlCheck.col(\"BACQADVCDTwo\"))\r\n&& (dfiMPC.col(\"MATL_GROUP\") === \"*\" || dfiMPC.col(\"MATL_GROUP\") === dfMatlCheck.col(\"MATL_GRP_4Two\"))\r\n&& (dfiMPC.col(\"BBAGITMID\") === \"*\" || dfiMPC.col(\"BBAGITMID\") === dfMatlCheck.col(\"BBAGITMIDTwo\"))\r\n&& (dfiMPC.col(\"BFRGNMLTY\") === \"*\" || dfiMPC.col(\"BFRGNMLTY\") === dfMatlCheck.col(\"DODAAC\")))\r\n.groupBy(\"MATERIAL\").min(\"BCNFGBMPC\") // these 4 categories should correspond to unique MATERIALs as defined\r\n\r\n*/\r\ndf.join(dfMatlRule.selectExpr(\"MATERIAL\", \"BCNFCHR15 as BMPC\"), Seq(\"MATERIAL\"), \"left\")\r\n}\r\n\r\ndef deriveBNMCS_IND(df: DataFrame): DataFrame = {\r\n  // ABAP string indexing starts at 0, Spark string indexing starts at 1\r\n  // substring is safe for nulls\r\n  df.withColumn(\"BNMCS_IND\", when((substring($\"BORGDNUM\", 1, 1) isin (\"B\", \"C\", \"L\", \"M\", \"W\")) &&\r\n  (substring($\"BNS_RDD\", 1, 1) isin (\"7\", \"9\", \"E\", \"N\")) && ($\"BPRI_CD\" isin \r\n  (\"01\", \"02\", \"03\", \"04\", \"05\", \"06\", \"07\", \"08\")), \"Y\").otherwise(\"N\"))\r\n}\r\n\r\ndef deriveBCASREP(df: DataFrame): DataFrame = {\r\n  df.withColumn(\"BCASREP\", when((substring($\"BORGDNUM\", 1, 1) isin (\"N\", \"P\", \"Q\", \"R\", \"V\", \"Z\")) && \r\n  (substring($\"BORGDNUM\", 11, 1) isin (\"W\", \"G\")) && \r\n  (substring($\"BNS_RDD\", 1, 1) isin  (\"7\", \"9\", \"E\", \"N\")) && ($\"BPRI_CD\" isin (\"01\", \"02\", \"03\")), \"Y\")\r\n  .otherwise(\"N\"))\r\n}\r\n\r\n\r\nval bofo12TransformedFinal = bofo12Two.transform(setBITMCATGP(dfMaterial))\r\n  .transform(deriveBFMSCSCD)\r\n  .transform(deriveBCONVERT)\r\n  .transform(deriveBIPG(dfIPGTwo))\r\n  .transform(deriveBORD_TYPE(dfORDTTwo))\r\n  .transform(deriveBMPC(dfMaterial, dfMPCTwo, dfDEPMAIN))\r\n  .transform(deriveBNMCS_IND)\r\n  .transform(deriveBCASREP)\r\n  .selectExpr(\"DOC_NUMBER\",\"S_ORD_ITEM\",\"BFMS_PRG\",\"BFMSCSCD\",\"BORDER_DT\",\"MIN_DL_QTY\",\"BASE_UOM AS BASE_UOM12\",\"LOWR_BND\",\"DOC_CURRCY AS DOC_CURRCY12\",\"BPURPRIC\",\"CML_CF_QTY\",\"CREATEDON\",\"NET_VALUE\",\"BCONVERT\",\"MATERIAL\",\"BIPG\",\"SUBTOTAL_3\",\"SUBTOTAL_4\",\"SUBTOTAL_5\",\"SUBTOTAL_6\",\"SUBTOTAL_2\",\"SUBTOTAL_1\",\"SOLD_TO\",\"REQDEL_QTY\",\"BCUSTPODT\",\"BORD_TYPE\",\"BBSTNK\",\"BORGDNUM\",\"BPRI_CD\",\"CML_OR_QTY\",\"DSDEL_DATE\",\"BMPC\",\"ITEM_CATEG\",\"REASON_REJ\",\"LOC_CURRCY\",\"UPPR_BND\",\"SALES_UNIT AS SALES_UNIT12\",\"BITMCATGP\",\"COND_PR_UN\",\"NET_PRICE\",\"BPONBR\",\"DOC_TYPE\",\"BMATRCVDT\",\"BNMCS_IND\",\"BCASREP\",\"BOFBNSRDD\",\"BOFITMCAT\",\"BOFPRICD\",\"BMRA_QTY\",\"CUST_GRP3\",\"BZZESD\",\"CML_CD_QTY\",\"BNS_RDD AS BNS_RDD12\").persist()\r\n  \r\n\r\n\r\n\r\n","user":"hac4796","dateUpdated":"2017-12-19T17:36:55-0500","config":{"editorSetting":{"language":"scala","editOnDblClick":false},"colWidth":12,"editorMode":"ace/mode/scala","editorHide":false,"results":{"0":{"graph":{"mode":"table","height":288,"optionOpen":false}}},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"res79: String = Running time - 0:18.565\nbofo12Two: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [DOC_NUMBER: string, S_ORD_ITEM: string ... 47 more fields]\ndfIPGTwo: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [BCNFGIPG: string, OBJVERS: string ... 3 more fields]\ndfMPCTwo: org.apache.spark.sql.DataFrame = [BCNFGBMPC: int, OBJVERS: string ... 7 more fields]\ndfORDTTwo: org.apache.spark.sql.DataFrame = [ITEM_CATEG: string, BMATCATGP: string ... 1 more field]\nuniqueMatl: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [MATERIAL: string, BDODAAC: string]\nres81: uniqueMatl.type = [MATERIAL: string, BDODAAC: string]\ndfMaterial: org.apache.spark.sql.DataFrame = [MATERIAL: string, BDODAAC: string ... 6 more fields]\nsetBITMCATGP: (dfMatl: org.apache.spark.sql.DataFrame)(df: org.apache.spark.sql.DataFrame)org.apache.spark.sql.DataFrame\nderiveBFMSCSCD: (df: org.apache.spark.sql.DataFrame)org.apache.spark.sql.DataFrame\nderiveBCONVERT: (df: org.apache.spark.sql.DataFrame)org.apache.spark.sql.DataFrame\nderiveBIPG: (dfiIPG: org.apache.spark.sql.DataFrame)(df: org.apache.spark.sql.DataFrame)org.apache.spark.sql.DataFrame\nderiveBORD_TYPE: (dfiORDT: org.apache.spark.sql.DataFrame)(df: org.apache.spark.sql.DataFrame)org.apache.spark.sql.DataFrame\nwindowSpecMatlRule: org.apache.spark.sql.expressions.WindowSpec = org.apache.spark.sql.expressions.WindowSpec@18e22493\nderiveBMPC: (dfMatlCheck: org.apache.spark.sql.DataFrame, dfiMPC: org.apache.spark.sql.DataFrame, dfiDEPMAIN: org.apache.spark.sql.DataFrame)(df: org.apache.spark.sql.DataFrame)org.apache.spark.sql.DataFrame\nderiveBNMCS_IND: (df: org.apache.spark.sql.DataFrame)org.apache.spark.sql.DataFrame\nderiveBCASREP: (df: org.apache.spark.sql.DataFrame)org.apache.spark.sql.DataFrame\nbofo12TransformedFinal: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [DOC_NUMBER: string, S_ORD_ITEM: string ... 51 more fields]\n"}]},"apps":[],"jobName":"paragraph_1512659779236_-1695556431","id":"20171020-042508_1511990341","dateCreated":"2017-12-07T10:16:19-0500","dateStarted":"2017-12-19T17:37:38-0500","dateFinished":"2017-12-19T17:39:33-0500","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:14803"},{"text":"%spark2\r\ngetTimeDifference(bofo20Start)\r\n// BOF_O14\r\ndef deriveYOBLOCK(df: DataFrame): DataFrame = {\r\n  df.withColumn(\"B_YOBLOCK\", when($\"B_YOBLOCK\" === \"Y\" || $\"DLV_BLOCK\" === \"Y0\", \"Y\").when($\"B_YOBLOCK\".isNotNull, \"N\")\r\n  .otherwise(\"\"))\r\n}\r\n\r\ndef deriveZTBLOCK(df: DataFrame): DataFrame = {\r\n  df.withColumn(\"B_ZTBLOCK\", when($\"B_ZTBLOCK\" === \"Y\" || $\"DLV_BLOCK\" === \"ZT\", \"Y\").when($\"B_YOBLOCK\".isNotNull, \"N\")\r\n  .otherwise(\"\"))\r\n}\r\n\r\n\r\nval io20: Option[DataFrame] = if(!bofo20.head(1).isEmpty) {\r\n  Some(bofo20.select($\"DOC_NUMBER\", $\"S_ORD_ITEM\", $\"B_YOBLOCK\", $\"B_ZTBLOCK\")\r\n  .join(bofo14.select(\"DOC_NUMBER\", \"S_ORD_ITEM\").sort(desc(\"DOC_NUMBER\"), desc(\"S_ORD_ITEM\")).dropDuplicates(\"DOC_NUMBER\", \"S_ORD_ITEM\"),  Seq(\"DOC_NUMBER\", \"S_ORD_ITEM\"), \"inner\"))\r\n} else None\r\n\r\n// pull from current bofo20 for comparison, DLV_BLOCK should be used from 14, not 20, I believe\r\n//val io20 = bofo20.select($\"DOC_NUMBER\", $\"S_ORD_ITEM\", $\"B_YOBLOCK\", $\"B_ZTBLOCK\")\r\n\r\n// set all RECORDMODEs to \"A\"\r\n// delete adjacent duplicates, see if this works\r\n// empty fields sort to the top\r\n// need to check what it sorts by\r\n// need fields from io20 to derive YOBLOCK and ZTBLOCK\r\nval bofo14Transformed = if (io20 != None) {\r\n  bofo14.withColumn(\"RECORDMODE\", lit(\"A\"))\r\n  .join(io20.get, Seq(\"DOC_NUMBER\", \"S_ORD_ITEM\"), \"left\")\r\n  .transform(deriveYOBLOCK)\r\n  .transform(deriveZTBLOCK)\r\n\r\n} else {\r\n  // since BOF_O20 doesn't exist on initial load, 3 things for yoblock and ztblock: doesn't join and is null, \r\n  // is N, unless dlv block indicates Y\r\n  bofo14.withColumn(\"RECORDMODE\", lit(\"A\"))\r\n  .withColumn(\"B_YOBLOCK\", lit(\"\"))\r\n  .withColumn(\"B_ZTBLOCK\", lit(\"\"))\r\n  .transform(deriveYOBLOCK)\r\n  .transform(deriveZTBLOCK)\r\n}\r\n\r\nval bofo14Transformed2 = bofo14Transformed.groupBy(\"DOC_NUMBER\",\"S_ORD_ITEM\").agg(sum($\"CONF_QTY\") as \"CONF_QTY\", sum($\"REQ_QTY\") as \"REQ_QTY\", sum($\"BDMDQTY\") as \"BDMDQTY\")\r\n\r\nval bofo14TransformedFinal = bofo14Transformed2.join(bofo14Transformed.sort(\"DOC_NUMBER\",\"S_ORD_ITEM\",\"SCHED_LINE\").dropDuplicates(\"DOC_NUMBER\",\"S_ORD_ITEM\").selectExpr(\"DOC_NUMBER\",\"S_ORD_ITEM\",\"SCHED_LINE\",\"BASE_UOM AS BASE_UOM14\",\"SALES_UNIT AS SALES_UNIT14\",\"BNS_RDD AS BNS_RDD14\",\"DOC_CURRCY AS DOC_CURRCY14\",\"UNIT\",\"B_YOBLOCK\",\"B_ZTBLOCK\",\"DLV_BLOCK\"), Seq(\"DOC_NUMBER\",\"S_ORD_ITEM\"), \"inner\").persist()\r\n\r\n","user":"hac4796","dateUpdated":"2017-12-19T17:36:55-0500","config":{"editorSetting":{"language":"scala","editOnDblClick":false},"colWidth":12,"editorMode":"ace/mode/scala","editorHide":false,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"res110: String = Running time - 1:58.304\nderiveYOBLOCK: (df: org.apache.spark.sql.DataFrame)org.apache.spark.sql.DataFrame\nderiveZTBLOCK: (df: org.apache.spark.sql.DataFrame)org.apache.spark.sql.DataFrame\nio20: Option[org.apache.spark.sql.DataFrame] = None\nbofo14Transformed: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [DOC_NUMBER: string, S_ORD_ITEM: string ... 13 more fields]\nbofo14Transformed2: org.apache.spark.sql.DataFrame = [DOC_NUMBER: string, S_ORD_ITEM: string ... 3 more fields]\nbofo14TransformedFinal: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [DOC_NUMBER: string, S_ORD_ITEM: string ... 12 more fields]\n"}]},"apps":[],"jobName":"paragraph_1512659779236_-1695556431","id":"20171020-101247_2132208969","dateCreated":"2017-12-07T10:16:19-0500","dateStarted":"2017-12-19T17:37:54-0500","dateFinished":"2017-12-19T17:41:17-0500","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:14804"},{"text":"%spark2\r\n// BOF_O15\r\n\r\n// overwrite RECORDMODE and derive GIS_QTY and QCOASREQ\r\ndef deriveGIS_QTY(df: DataFrame): DataFrame = {\r\n  df.withColumn(\"GIS_QTY\", when($\"GI_STS\" === \"C\", $\"DLV_QTY\").otherwise(\"\"))\r\n}\r\n\r\ndef deriveQCOASREQ(df: DataFrame): DataFrame = {\r\n  df.withColumn(\"QCOASREQ\", when($\"RECORDMODE\" === \"\" && $\"REC_DELETD\" =!= \"X\", \r\n  when($\"REQU_QTY\" > $\"CONF_QTY\", $\"CONF_QTY\").otherwise($\"REQU_QTY\"))\r\n  .when($\"RECORDMODE\" === \"X\" && $\"REC_DELETD\" === \"X\",\r\n  when($\"CONF_QTY\" > $\"REQU_QTY\", $\"CONF_QTY\").otherwise($\"REQU_QTY\")).otherwise(\"\"))\r\n}\r\n\r\nval bofo15Transformed = bofo15.withColumn(\"RECORDMODE\", when($\"RECORDMODE\" === \" \" && $\"REC_DELETD\" === \"X\", \"A\").otherwise($\"RECORDMODE\"))\r\n  .withColumn(\"CONF_QTY\", $\"CONF_QTY\".cast(\"decimal\"))\r\n  .withColumn(\"REQU_QTY\", $\"REQU_QTY\".cast(\"decimal\"))\r\n  .transform(deriveGIS_QTY)\r\n  .transform(deriveQCOASREQ)\r\n  \r\nval bofo15Transformed2 = bofo15Transformed.groupBy(\"DOC_NUMBER\",\"S_ORD_ITEM\").agg(sum($\"BCNDQTY\") as \"BCNDQTY\", sum($\"GIS_QTY\") as \"GIS_QTY\", sum($\"QCOASREQ\") as \"QCOASREQ\",sum($\"DLVQEYCR\") as \"DLVQEYCR\",sum($\"DLVQEYSC\") as \"DLVQEYSC\",sum($\"DLVQLECR\") as \"DLVQLECR\",sum($\"DLVQLESC\") as \"DLVQLESC\")\r\n\r\nval bofo15TransformedFinal = bofo15Transformed2.dropDuplicates(\"DOC_NUMBER\",\"S_ORD_ITEM\").select(\"DOC_NUMBER\",\"S_ORD_ITEM\",\"BCNDQTY\",\"GIS_QTY\",\"QCOASREQ\",\"DLVQEYCR\",\"DLVQEYSC\",\"DLVQLECR\",\"DLVQLESC\")\r\n","user":"hac4796","dateUpdated":"2017-12-19T17:36:55-0500","config":{"editorSetting":{"language":"scala","editOnDblClick":false},"colWidth":12,"editorMode":"ace/mode/scala","editorHide":false,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"deriveGIS_QTY: (df: org.apache.spark.sql.DataFrame)org.apache.spark.sql.DataFrame\nderiveQCOASREQ: (df: org.apache.spark.sql.DataFrame)org.apache.spark.sql.DataFrame\nbofo15Transformed: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [DOC_NUMBER: string, S_ORD_ITEM: string ... 13 more fields]\nbofo15Transformed2: org.apache.spark.sql.DataFrame = [DOC_NUMBER: string, S_ORD_ITEM: string ... 7 more fields]\nbofo15TransformedFinal: org.apache.spark.sql.DataFrame = [DOC_NUMBER: string, S_ORD_ITEM: string ... 7 more fields]\n"}]},"apps":[],"jobName":"paragraph_1512659779237_-1695941180","id":"20171020-101304_1431999580","dateCreated":"2017-12-07T10:16:19-0500","dateStarted":"2017-12-19T17:39:34-0500","dateFinished":"2017-12-19T17:41:18-0500","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:14805"},{"text":"%spark2\r\n// BOF_O17\r\n//needs commenting\r\n\r\ndef clearReversalCharacteristics(df: DataFrame): DataFrame = {\r\n  df.withColumn(\"LOAD_DATE\", when($\"RECORDMODE\" === \"R\", \"\").otherwise($\"LOAD_DATE\"))\r\n  .withColumn(\"ACT_GI_DTE\", when($\"RECORDMODE\" === \"R\", \"\").otherwise($\"ACT_GI_DTE\"))\r\n  .withColumn(\"BSHIPTYP\", when($\"RECORDMODE\" === \"R\", \"\").otherwise($\"BSHIPTYP\"))\r\n  .withColumn(\"RECORDMODE\", when($\"RECORDMODE\" === \"R\", \"A\").otherwise($\"RECORDMODE\"))\r\n}\r\n\r\nval bofo17Transformed = bofo17.transform(clearReversalCharacteristics).filter($\"PRVDOC_CTG\" === \"C\" || $\"PRVDOC_CTG\" === \"I\") \r\nval bofo17Transformed2 = bofo17Transformed.groupBy(\"REFER_DOC\",\"REFER_ITM\").agg(sum($\"DLV_QTY\") as \"DLV_QTY\")\r\nval bofo17Transformed3 = bofo17Transformed2.selectExpr(\"REFER_DOC AS DOC_NUMBER\",\"REFER_ITM AS S_ORD_ITEM\", \"DLV_QTY\")\r\nval bofo17TransformedFinal = bofo17Transformed3.join(bofo17Transformed.dropDuplicates(\"REFER_DOC\",\"REFER_ITM\").selectExpr(\"REFER_DOC AS DOC_NUMBER\",\"REFER_ITM AS S_ORD_ITEM\",\"LOAD_DATE\",\"BSHIPTYP\",\"ACT_GI_DTE\",\"CREATEDON AS DOC_DATE\"), Seq(\"DOC_NUMBER\", \"S_ORD_ITEM\"), \"inner\")\r\n","user":"hac4796","dateUpdated":"2017-12-19T17:36:55-0500","config":{"editorSetting":{"language":"scala","editOnDblClick":false},"colWidth":12,"editorMode":"ace/mode/scala","editorHide":false,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"clearReversalCharacteristics: (df: org.apache.spark.sql.DataFrame)org.apache.spark.sql.DataFrame\nbofo17Transformed: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [DELIV_NUMB: string, DELIV_ITEM: string ... 32 more fields]\nbofo17Transformed2: org.apache.spark.sql.DataFrame = [REFER_DOC: string, REFER_ITM: string ... 1 more field]\nbofo17Transformed3: org.apache.spark.sql.DataFrame = [DOC_NUMBER: string, S_ORD_ITEM: string ... 1 more field]\nbofo17TransformedFinal: org.apache.spark.sql.DataFrame = [DOC_NUMBER: string, S_ORD_ITEM: string ... 5 more fields]\n"}]},"apps":[],"jobName":"paragraph_1512659779237_-1695941180","id":"20171020-101307_1213053741","dateCreated":"2017-12-07T10:16:19-0500","dateStarted":"2017-12-19T17:41:17-0500","dateFinished":"2017-12-19T17:41:19-0500","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:14806"},{"text":"%spark2\r\n// BOF_O21\r\n\r\nval bofo21Two = bofo21.filter($\"DOC_NUMBER\".isNotNull && $\"S_ORD_ITEM\".isNotNull)\r\n\r\n\r\nval nonreversals = bofo21Two.filter(!($\"RECORDMODE\" <=> \"R\"))\r\nval reversals = bofo21Two.filter($\"RECORDMODE\" === \"R\")\r\n\r\nval bofo21Transformed = if (!reversals.take(1).isEmpty) {\r\n\r\n// does sort handle dates?\r\n  val iPOData = bofo21Two.select(\"DOC_NUMBER\", \"S_ORD_ITEM\", \"BCREATEDT\", \"PSTNG_DATE\", \"CONF_DATE\",\r\n    \"BCAGECDPN\", \"BIDNLF\", \"BMFRPN\", \"SCL_DELDAT\").join(reversals.select(\"DOC_NUMBER\", \"S_ORD_ITEM\"), Seq(\"DOC_NUMBER\", \"S_ORD_ITEM\"), \"inner\")\r\n    .sort(desc(\"BCREATEDT\")).dropDuplicates(\"BCREATEDT\")\r\n\r\n// clear out characteristic values if BCREATEDT is Null\r\n  reversals.select(\"DOC_NUMBER\", \"S_ORD_ITEM\", \"GR_QTY\", \"PCONF_QTY\", \"BPLT\", \"B_ALT\", \"RECORDMODE\")\r\n  .join(iPOData, Seq(\"DOC_NUMBER\", \"S_ORD_ITEM\"), \"left\")\r\n\r\n  nonreversals.union(reversals)\r\n} else nonreversals\r\n\r\n   // clear out characteristic values if BCREATEDT is Null\r\nval bofo21Transformed2 = bofo21Transformed.withColumn(\"RECORDMODE\", when($\"RECORDMODE\" === \"R\", \"A\").otherwise($\"RECORDMODE\"))\r\n  .withColumn(\"PSTNG_DATE\", when($\"BCREATEDT\".isNull, \"\").otherwise($\"PSTNG_DATE\"))\r\n  .withColumn(\"CONF_DATE\", when($\"BCREATEDT\".isNull, \"\").otherwise($\"CONF_DATE\"))\r\n  .withColumn(\"BCAGECDPN\", when($\"BCREATEDT\".isNull, \"\").otherwise($\"BCAGECDPN\"))\r\n  .withColumn(\"BIDNLF\", when($\"BCREATEDT\".isNull, \"\").otherwise($\"BIDNLF\"))\r\n  .withColumn(\"BMFRPN\", when($\"BCREATEDT\".isNull, \"\").otherwise($\"BMFRPN\"))\r\n  .withColumn(\"SCL_DELDAT\", when($\"BCREATEDT\".isNull, \"\").otherwise($\"SCL_DELDAT\"))\r\n  .groupBy(\"DOC_NUMBER\",\"S_ORD_ITEM\").agg(sum($\"GR_QTY\") as \"GR_QTY\", sum($\"PCONF_QTY\") as \"PCONF_QTY\", sum($\"BPLT\") as \"BPLT\", sum($\"B_ALT\") as \"B_ALT\")\r\n\r\nval bofo21TransformedFinal = bofo21Transformed2.join(bofo21Transformed.dropDuplicates(\"DOC_NUMBER\",\"S_ORD_ITEM\").selectExpr(\"DOC_NUMBER\",\"S_ORD_ITEM\",\"PSTNG_DATE AS BGR_DATE\",\"CONF_DATE\",\"BCAGECDPN\",\"BIDNLF\",\"BMFRPN\",\"BCREATEDT\",\"SCL_DELDAT\"), Seq(\"DOC_NUMBER\",\"S_ORD_ITEM\"), \"inner\")\r\n\r\n","user":"hac4796","dateUpdated":"2017-12-19T17:36:55-0500","config":{"editorSetting":{"language":"scala","editOnDblClick":false},"colWidth":12,"editorMode":"ace/mode/scala","editorHide":false,"results":{"0":{"graph":{"mode":"table","height":294,"optionOpen":false}}},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"bofo21Two: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [DOC_NUMBER: string, S_ORD_ITEM: string ... 14 more fields]\nnonreversals: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [DOC_NUMBER: string, S_ORD_ITEM: string ... 14 more fields]\nreversals: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [DOC_NUMBER: string, S_ORD_ITEM: string ... 14 more fields]\nbofo21Transformed: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [DOC_NUMBER: string, S_ORD_ITEM: string ... 14 more fields]\nbofo21Transformed2: org.apache.spark.sql.DataFrame = [DOC_NUMBER: string, S_ORD_ITEM: string ... 4 more fields]\nbofo21TransformedFinal: org.apache.spark.sql.DataFrame = [DOC_NUMBER: string, S_ORD_ITEM: string ... 11 more fields]\n"}]},"apps":[],"jobName":"paragraph_1512659779237_-1695941180","id":"20171006-091654_311577244","dateCreated":"2017-12-07T10:16:19-0500","dateStarted":"2017-12-19T17:41:18-0500","dateFinished":"2017-12-19T17:41:20-0500","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:14807"},{"text":"%spark2\nval joined = bofo14TransformedFinal\n.join(bofo15TransformedFinal, Seq(\"DOC_NUMBER\", \"S_ORD_ITEM\"), \"outer\")\n.join(bofo17TransformedFinal, Seq(\"DOC_NUMBER\", \"S_ORD_ITEM\"), \"outer\")\n.join(bofo21TransformedFinal, Seq(\"DOC_NUMBER\", \"S_ORD_ITEM\"), \"outer\")\n.join(bofo13, Seq(\"DOC_NUMBER\", \"S_ORD_ITEM\"), \"outer\")\n.join(bofo12TransformedFinal, Seq(\"DOC_NUMBER\", \"S_ORD_ITEM\"), \"outer\")\n.join(zarixsd2, Seq(\"DOC_NUMBER\", \"S_ORD_ITEM\"), \"left\")\n.withColumn(\"BFMS_IND\",lit(\"\"))\n.withColumn(\"BMILSVC_1\",lit(\"\"))\n.withColumn(\"BMILSVC_2\",lit(\"\"))\n.withColumn(\"BMILSVC_G\",lit(\"\"))\n.withColumn(\"BMILSVC_S\",lit(\"\"))\n.withColumn(\"RTPLCST\",lit(0).cast(StringType))\n.withColumn(\"RECORDMODE\",lit(\"\"))\n.withColumn(\"BORGNLDOC\",lit(\"\"))\n.withColumn(\"BACQADVCD\",lit(\"\"))\n.withColumn(\"PO_UNIT\", lit(\"\"))\n.withColumn(\"BASE_UOM\",coalesce($\"BASE_UOM12\",$\"BASE_UOM14\"))\n.withColumn(\"DOC_CURRCY\",coalesce($\"DOC_CURRCY12\",$\"DOC_CURRCY14\"))\n.withColumn(\"SALES_UNIT\",coalesce($\"SALES_UNIT12\",$\"SALES_UNIT14\"))\n.withColumn(\"BNS_RDD\",coalesce($\"BNS_RDD12\",$\"BNS_RDD14\"))\n.drop($\"BNS_RDD12\")\n.drop($\"BNS_RDD14\")\n.drop($\"BASE_UOM12\")\n.drop($\"BASE_UOM14\")\n.drop($\"DOC_CURRCY12\")\n.drop($\"DOC_CURRCY14\")\n.drop($\"SALES_UNIT12\")\n.drop($\"SALES_UNIT14\")\n.persist()\n\n\n","user":"hac4796","dateUpdated":"2017-12-19T17:36:55-0500","config":{"editorSetting":{"language":"scala","editOnDblClick":false},"colWidth":12,"editorMode":"ace/mode/scala","editorHide":false,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"joined: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [DOC_NUMBER: string, S_ORD_ITEM: string ... 98 more fields]\n"}]},"apps":[],"jobName":"paragraph_1512659779237_-1695941180","id":"20171122-152011_1044102892","dateCreated":"2017-12-07T10:16:19-0500","dateStarted":"2017-12-19T17:41:19-0500","dateFinished":"2017-12-19T17:41:23-0500","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:14808"},{"text":"%spark2\n/*\nval joined = bofo14Transformed.dropDuplicates(\"DOC_NUMBER\",\"S_ORD_ITEM\")\n.join(bofo15Transformed.dropDuplicates(\"DOC_NUMBER\",\"S_ORD_ITEM\"), Seq(\"DOC_NUMBER\", \"S_ORD_ITEM\"), \"outer\")\n.join(bofo17Transformed.dropDuplicates(\"DOC_NUMBER\",\"S_ORD_ITEM\"), Seq(\"DOC_NUMBER\", \"S_ORD_ITEM\"), \"outer\")\n.join(bofo21Transformed.dropDuplicates(\"DOC_NUMBER\",\"S_ORD_ITEM\"), Seq(\"DOC_NUMBER\", \"S_ORD_ITEM\"), \"outer\")\n.join(bofo13.dropDuplicates(\"DOC_NUMBER\",\"S_ORD_ITEM\"), Seq(\"DOC_NUMBER\", \"S_ORD_ITEM\"), \"outer\")\n.join(bofo12Transformed.dropDuplicates(\"DOC_NUMBER\",\"S_ORD_ITEM\"), Seq(\"DOC_NUMBER\", \"S_ORD_ITEM\"), \"outer\")\n.withColumn(\"BFMS_IND\",lit(\"\"))\n.withColumn(\"BMILSVC_1\",lit(\"\"))\n.withColumn(\"BMILSVC_2\",lit(\"\"))\n.withColumn(\"BMILSVC_G\",lit(\"\"))\n.withColumn(\"BMILSVC_S\",lit(\"\"))\n.withColumn(\"RTPLCST\",lit(0).cast(StringType))\n.withColumn(\"RECORDMODE\",lit(\"\"))\n.withColumn(\"BORGNLDOC\",lit(\"\"))\n.withColumn(\"BACQADVCD\",lit(\"\"))\n.withColumn(\"BASE_UOM\",coalesce($\"BASE_UOM12\",$\"BASE_UOM14\"))\n.withColumn(\"DOC_CURRCY\",coalesce($\"DOC_CURRCY12\",$\"DOC_CURRCY14\"))\n.withColumn(\"SALES_UNIT\",coalesce($\"SALES_UNIT12\",$\"SALES_UNIT14\"))\n.withColumn(\"BNS_RDD\",coalesce($\"BNS_RDD12\",$\"BNS_RDD14\"))\n.drop($\"BNS_RDD12\")\n.drop($\"BNS_RDD14\")\n.drop($\"BASE_UOM12\")\n.drop($\"BASE_UOM14\")\n.drop($\"DOC_CURRCY12\")\n.drop($\"DOC_CURRCY14\")\n.drop($\"SALES_UNIT12\")\n.drop($\"SALES_UNIT14\")\n.persist()\n\n\n*/","user":"hac4796","dateUpdated":"2017-12-19T17:36:55-0500","config":{"editorSetting":{"language":"scala","editOnDblClick":false},"colWidth":12,"editorMode":"ace/mode/scala","editorHide":false,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[]},"apps":[],"jobName":"paragraph_1512659779237_-1695941180","id":"20171109-141448_1384960988","dateCreated":"2017-12-07T10:16:19-0500","dateStarted":"2017-12-19T17:41:21-0500","dateFinished":"2017-12-19T17:41:24-0500","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:14809"},{"text":"%spark2\n \n//bofOrderedFields20.split(\",\").toSeq\n//val bof20FieldsFinal = bofOrderedFields20.split(\",\").toSeq\n\nval finaldf = joined.select(col(\"DOC_NUMBER\"),col(\"S_ORD_ITEM\"),col(\"BFMS_PRG\"),col(\"BFMSCSCD\"),col(\"BFMS_IND\"),col(\"BMILSVC_1\"),col(\"BMILSVC_2\"),col(\"BMILSVC_G\"),col(\"BMILSVC_S\"),col(\"BORDER_DT\"),col(\"MIN_DL_QTY\"),col(\"BASE_UOM\"),col(\"LOWR_BND\"),col(\"DOC_CURRCY\"),col(\"BPURPRIC\"),col(\"RTPLCST\"),col(\"CML_CF_QTY\"),col(\"BTORDQTY\"),col(\"BTKILLQTY\"),col(\"BTKILLUOM\"),col(\"BTVRSTCD\"),col(\"CREATEDON\"),col(\"CONF_QTY\"),col(\"NET_VALUE\"),col(\"REQ_QTY\"),col(\"BCNDQTY\"),col(\"BDMDQTY\"),col(\"GIS_QTY\"),col(\"BCONVERT\"),col(\"MATERIAL\"),col(\"BIPG\"),col(\"SUBTOTAL_3\"),col(\"SUBTOTAL_4\"),col(\"SUBTOTAL_5\"),col(\"SUBTOTAL_6\"),col(\"SUBTOTAL_2\"),col(\"SUBTOTAL_1\"),col(\"QCOASREQ\"),col(\"LOAD_DATE\"),col(\"SOLD_TO\"),col(\"DLV_QTY\"),col(\"REQDEL_QTY\"),col(\"BGR_DATE\"),col(\"RECORDMODE\"),col(\"BCUSTPODT\"),col(\"BNS_RDD\"),col(\"BORD_TYPE\"),col(\"BBSTNK\"),col(\"BORGDNUM\"),col(\"BORGNLDOC\"),col(\"BPRI_CD\"),col(\"CML_OR_QTY\"),col(\"DSDEL_DATE\"),col(\"BMPC\"),col(\"ITEM_CATEG\"),col(\"REASON_REJ\"),col(\"LOC_CURRCY\"),col(\"UPPR_BND\"),col(\"SALES_UNIT\"),col(\"GR_QTY\"),col(\"BSHIPTYP\"),col(\"BITMCATGP\"),col(\"BTSALEUNT\"),col(\"COND_PR_UN\"),col(\"NET_PRICE\"),col(\"BPONBR\"),col(\"ACT_GI_DTE\"),col(\"DOC_TYPE\"),col(\"PO_UNIT\"),col(\"UNIT\"),col(\"PCONF_QTY\"),col(\"CML_CD_QTY\"),col(\"CONF_DATE\"),col(\"BCAGECDPN\"),col(\"BIDNLF\"),col(\"BMFRPN\"),col(\"DLVQEYCR\"),col(\"DLVQEYSC\"),col(\"DLVQLECR\"),col(\"BMATRCVDT\"),col(\"BNMCS_IND\"),col(\"BCASREP\"),col(\"DLVQLESC\"),col(\"BCREATEDT\"),col(\"DOC_DATE\"),col(\"B_YOBLOCK\"),col(\"B_ZTBLOCK\"),col(\"BOFBNSRDD\"),col(\"BOFITMCAT\"),col(\"BOFPRICD\"),col(\"BPLT\"),col(\"BACQADVCD\"),col(\"SCL_DELDAT\"),col(\"B_ALT\"),col(\"DLV_BLOCK\"),col(\"BMRA_QTY\"),col(\"CUST_GRP3\"),col(\"BZZESD\"),col(\"BEARCDTE\"))\n\n\nfinaldf.persist(org.apache.spark.storage.StorageLevel.MEMORY_ONLY_SER)\nval writepath = \"/data/dataset/enrichment/txn_proc/bof_o20/current\"\nIngestAndWrite.writeEntity(finaldf, writepath, \"overwrite\", \"csv\")\nbofo14TransformedFinal.unpersist()\njoined.unpersist()\nfinaldf.unpersist()\ngetTimeDifference(bofo20Start)","user":"hac4796","dateUpdated":"2017-12-19T17:36:55-0500","config":{"editorSetting":{"language":"scala","editOnDblClick":false},"colWidth":12,"editorMode":"ace/mode/scala","editorHide":false,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"finaldf: org.apache.spark.sql.DataFrame = [DOC_NUMBER: string, S_ORD_ITEM: string ... 97 more fields]\nres152: finaldf.type = [DOC_NUMBER: string, S_ORD_ITEM: string ... 97 more fields]\nwritepath: String = /data/dataset/enrichment/txn_proc/bof_o20/current\nres154: bofo14TransformedFinal.type = [DOC_NUMBER: string, S_ORD_ITEM: string ... 12 more fields]\nres155: joined.type = [DOC_NUMBER: string, S_ORD_ITEM: string ... 98 more fields]\nres156: finaldf.type = [DOC_NUMBER: string, S_ORD_ITEM: string ... 97 more fields]\nres157: String = Running time - 4:43.623\n"}]},"apps":[],"jobName":"paragraph_1512659779238_-1694786933","id":"20171113-145641_2095390216","dateCreated":"2017-12-07T10:16:19-0500","dateStarted":"2017-12-19T17:41:24-0500","dateFinished":"2017-12-19T17:42:19-0500","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:14810"},{"text":"","user":"hac4796","dateUpdated":"2017-12-19T17:36:55-0500","config":{"colWidth":12,"editorMode":"ace/mode/scala","results":{},"enabled":true,"editorSetting":{"language":"scala","editOnDblClick":false},"editorHide":false},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1512659779238_-1694786933","id":"20171130-165949_252485224","dateCreated":"2017-12-07T10:16:19-0500","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:14811"}],"name":"/Testing/BOFO20","id":"2D38HJND1","angularObjects":{"2D29GT85T:shared_process":[],"2CHS8UYQQ:shared_process":[],"2C4U48MY3_spark2:hac4796:":[],"2CZC46K15:shared_process":[],"2C8A4SZ9T_livy2:hac4796:":[],"2CKAY1A8Y:shared_process":[]},"config":{"looknfeel":"default","personalizedMode":"false"},"info":{}}
JSON Template

3 Space Tab
JSON Specification

RFC 8259
Fix JSON 

#1
VALID (RFC 8259)
Formatted JSON Data
{
   "paragraphs":[
      {
         "text":"%spark2\n\nimport org.apache.spark.sql.types._\nimport org.apache.spark.sql.{Column, DataFrame, Row, SaveMode}\nimport org.apache.spark.sql.functions._\nimport scala.util.Try\nimport spark.implicits._\nimport org.apache.hadoop.fs.FileSystem\nimport sys.process._\nimport org.joda.time.format._\nimport java.sql.{Date, Timestamp}\nimport java.text.SimpleDateFormat\nimport org.apache.spark.sql.catalyst.util.DateTimeUtils\nimport org.apache.spark.sql.functions.udf\nimport org.apache.spark.sql.catalyst.ScalaReflection\nimport org.apache.spark.sql.expressions.Window\nimport java.util.Calendar\n\ndef getCurrentTime = {\n    new SimpleDateFormat(\"yyyy-MM-dd HH:mm:ss.SSS\").format(Calendar.getInstance().getTime())\n}\n\ndef getTimeDifference(startTime: String) = {\n  val format: SimpleDateFormat = new SimpleDateFormat(\"yyyy-MM-dd HH:mm:ss.SSS\");\n    val date1: java.util.Date = format.parse(startTime);\n    val date2 = format.parse(getCurrentTime);\n    val difference = date2.getTime() - date1.getTime(); \n    \"Running time - \" + (difference / 1000 / 60) + \":\" + (\"0\" + ((difference * .001) % 60 ).toInt).takeRight(2) + \".\" + (difference % 1000) \n}\n\nval bofo20Start = getCurrentTime\n\n/* Object IngestAndWrite\n*\n*  This object will encapsulate the logic for reading and writing the HDFS\n*  \n*  Methods - loadSchema, loadEntity, writeEntity\n*\n*  Functions - schemaMapper\n*/\n\nobject IngestAndWrite {\n   /*\n    * Method - LoadSchema \n    *\n    * Parameters - inKeys:String, \n    *              inDateFields:String, \n    *              inIntFields:String, \n    *              inTimestampFields:String,\n    *              inDecimalFields:String, \n    *              inAllFieldsInOrder:String\n    *\n    * Return Type -Tuple6(  Keys:List[String], \n    *                       DateFields:List[String], \n    *                       IntFields:List[String], \n    *                       TimestampFields:List[String],\n    *                       DecimalFieldsList[String], \n    *                       AllFieldsInOrderList[String] \n    *                    )\n    *\n    * This method will read in a comma delimeted string for the key fields, the date fields, integer fields, timestamp fields, decimal fields, and all the fields\n    * If there are other data types found after bofo08 then we need to add those as well if they arent already covered.  For example, Strings cover 'CHAR' and 'UNIT' so we don't need a second data type.\n    * Also, the string data type is the most comon so that is the default and does not need to be specified.  \n    */\n    def loadSchema(  inKeys: String\n        , inDateFields: String\n        , inIntFields: String\n        , inTimestampFields: String\n        , inDecimalFields: String)(inAllFieldsInOrder: String): (List[String], List[String], List[String], List[String], List[String], List[String]) = \n        {( \n      inKeys.split(\",\").toList\n      , inDateFields.split(\",\").toList\n      , inIntFields.split(\",\").toList\n      , inTimestampFields.split(\",\").toList\n      , inDecimalFields.split(\",\").toList\n      , inAllFieldsInOrder.split(\",\").toList \n        )}\n    /*\n    * Function - schemaMapper\n    * Parameters - fieldName:String\n    *              isDeltaQueue: Boolean,\n    *              keys:List[String], \n    *              dateFields:List[String], \n    *              intFields:List[String], \n    *              timestampFields:List[String],\n    *              decimalFieldsList[String]\n    *\n    * Return Type - schema:StructField\n    *\n    * Used to map the field name to the proper data type based on the the user provided strings.  \n    * This function is designed to use the output of loadSchema as the input. \n    * The function will be used to interate through a the list of fields\n    */\n    \n    def schemaMapper : ( String \n       , List[String] \n       , List[String]\n       , List[String]\n       , List[String] \n       , List[String]) => StructField = \n       ( fieldName: String\n       , keys : List[String]\n       , dateFields : List[String]\n       , intFields : List[String]\n       , timestampFields : List[String]\n       , decimalFields : List[String]\n       )              => fieldName match {\n        case x if keys.contains(x) && intFields.contains(x) => StructField(fieldName, IntegerType, nullable =  false)\n        case x if keys.contains(x) => StructField(fieldName, StringType, nullable =  true)\n        case x if intFields.contains(x) => StructField(fieldName, IntegerType, nullable =  true)\n        case _ => StructField(fieldName, StringType, nullable =  false)\n        \n        }\n        \n        \n    /*\n    * Method - LoadEntity\n    * Parameters - schema:StructType, fullyQualifiedPath:String, formatType: String\n    * \n    * Return Type - Try[DataFrame]\n    *\n    * This method takes in a string and a schema.  The schema will be from the manually create comma delimeted strings  passed to loadschema.  Then Lists of strings are used to create the schema.  \n    * The path of the load file is hardcoded for now but can be added as parameter\n    */\n     def loadEntity(schema: StructType, fullyQualifiedPath: String, formatType: String): Try[DataFrame] = Try({\n         spark.read.format(formatType)\n        .option(\"delimiter\",\"\\t\")\n        .option(\"header\", \"false\")\n        .schema(schema)\n        .load(fullyQualifiedPath)\n        })//loadEntity end\n    \n    \n    \n    /*\n    * Method - writeEntity\n    * Parameters -outputDF: DataFrame, path: String, writeMode: String, formatType: String\n    * \n    * Return Type - Unit\n    *\n    * Writes the dataframe to path given in HDFS, using the mode 'writeMode'\n    * _SUCCESS file that is created will be deleted \n    *\n    * This write to different formats*/\n    def writeEntity(outputDF: DataFrame, path: String, writeMode: String, formatType: String): Unit =  { \n         outputDF.write\n        .mode(writeMode)\n        .option(\"delimiter\",\"\\t\")\n        .option(\"header\", \"false\")\n        .format(formatType).save(path)\n        \n        val fs:FileSystem = FileSystem.get(new java.net.URI(path + \"/_SUCCESS\"), sc.hadoopConfiguration)\n        fs.delete(new org.apache.hadoop.fs.Path(path + \"/_SUCCESS\"), false)\n    }//writeEntity end\n\n    def dynamicPartitionNumber(multiplierInt: Int) = {\n        val cores = spark.sparkContext.getConf.get(\"spark.executor.cores\").toInt \n        val allExecutors = spark.sparkContext.getExecutorMemoryStatus\n        val driver = spark.sparkContext.getConf.get(\"spark.driver.host\")\n          allExecutors.filter(! _._1.split(\":\")(0).equals(driver)).toList.length * multiplierInt * cores\n    }\n\n\n\n}//end  IngestAndWrite\n\n/* Object ProcessBOF\n*\n*  This object will encapsulate the logic for processing the BOF.  The methods for each BOF will be different\n*  \n*  Methods - updateTran01, updateTran01AndSaveErrorDF, joinBof08AndTran01\n*/\nobject ProcessBOF extends Serializable {\n\n    /*\n    * Method     - unallowable\n    * Parameters - rawString: String\n    * \n    * Return Type - Boolean\n    *\n    * This method checks the string column values for any unallowable character defined in the list.\n    * If any unallowable character is found then the method will return True else returns False.\n    */\n    def unallowable: String => Boolean = (rawString: String) => {\n        \n       //Unallowable character list \n       //$!@#%^&*()_+=-{}[]|\\:\";'<>?,./~`\n        \n        \n        val list = List(\"$\",\"!\",\"@\",\"#\",\"%\",\"^\",\"&\",\"*\",\"(\",\")\",\"_\",\"+\",\"=\",\"-\",\"{\",\"}\",\"[\",\"]\",\"|\",\"\"\"\\\"\"\",\":\",\"\"\"\"\"\"\",\";\",\"'\",\"<\",\">\",\"?\",\",\",\".\",\"/\",\"~\",\"`\")\n        if (rawString == null) {\n            false\n        } else {\n            list.exists(rawString.contains)  \n        }\n    } \n\n    /*\n    val unallowableUDF = udf[Boolean, String](unallowable)\n    */\n\n    /*\n    * Method - updateDeltaQueue\n    * Parameters - deltaQueueDF: DataFrame\n    * \n    * Return Type - Try[DataFrame]\n    *\n    * This method updates the delata queue dataframe based on the transformation logic.  \n    * The transformed dataframe is returned.\n    * This needs to change completely per BOF and DQ\n    */    \n    def updateDeltaQueue(deltaQueueDF: DataFrame): Try[DataFrame] = Try({\n        deltaQueueDF\n    })\n\n    \n    \n    /*\n    * Method - massageDeltaQueueAndReturnErrorDF\n    * Parameters - deltaQueueDF: DataFrame, inDates: String, badCharacterCols: String\n    * \n    * Return Type - Try[Tuple2(DataFrame, DataFrame)]\n    *\n    * This method updates the delta queue and also creates the error dataframe.\n    * All the fields for bad character checking and dates can be passed in as two string arguemnts and they will be handled dynamically\n    * 1. DataFrame `allColumnDF` selects all columns from incoming deltaQueue DataFrame and applies functions and return new columns and old.\n    * 2. DataFrame `massagedDF`  selects only the columns that will be used for downstream processing. Dates of 'string' datatype and ErrorChecks will be left behind.\n    * 3. DataFrame `errorDF`  selects all columns including newly created error columns and old raw columns.\n    * A tuple is returned with the updated delta queue and the error dataframe.\n    */\n    spark.udf.register(\"unallowableUDF\", unallowable)\n    def massageDeltaQueueAndReturnErrorDF(deltaQueueDF: DataFrame, inDates: String, badCharacterCols: String): Try[(DataFrame, Option[DataFrame])] = Try({\n        (deltaQueueDF, None)\n    })\n    \n    \n        \n    /*\n    * Method - joinBofAndDeltaQueue\n    * Parameters - deltaQueueDF: DataFrame, bofDF: DataFrame, keys: Array[String]\n    * \n    * Return Type - Try[DataFrame]\n    *\n    * This method merges the bof and the delta queue (tran01DF), the assumption is that the keys to join are the same name of columns\n    * A dataframe is returned that is the intersect of bof:DataFrame and tran01DF:DataFrame unioned with the set difference between bof:DataFrame and tran01DF:DataFrame\n    */\n    def joinBofAndDeltaQueue(deltaQueueDF: DataFrame, bofDF: DataFrame, keys: Array[String]): Try[DataFrame] = Try({\n            \n            val joinExprs = keys\n                           .map{ case (c) => deltaQueueDF(c) <=> bofDF(c) }\n                           .reduce(_ && _)\n             \n            bofDF.join(deltaQueueDF, joinExprs, \"outer\")\n                    .select(bofDF.columns.map(c => if (!(deltaQueueDF.columns contains \"$c\")) bofDF.col(s\"$c\") else coalesce(deltaQueueDF.col(s\"$c\"), bofDF.col(s\"$c\")) as c): _*)\n\n        })\n\n}//ProcessBOF\n\n/* Object Zarixsd2\n*\n*  This object will encapsulate the logic for processing the archive index.  The keys per BOF will be different, need to think about a way to pass thme dynamically\n*  \n*  Methods - loadZarixsd2, mergeBofWithZarixsd2\n*/\nobject Zarixsd2 {\n\n   \n    /*\n    * Method - loadZarixsd2\n    * \n    * Parameters - path: String\n    * \n    * Return Type - Try[DataFrame]\n    *\n    * This will load the archive table given the strings above in the Zarixsd2 object\n    * Uses the IngestAndWrite methods loadSchema and the function schemaMapper\n    */   \n    def loadZarixsd2(path: String) : Try[DataFrame] = {\n        /*\n        * These fields below cann be hard coded because there is only one Zarixsd2.  \n        * More dataytpes need to be handled though\n        */        \n        val keysString = \"DOC_NUMBER,S_ORD_ITEM\"\n        val datesString = \"BEARCDTE\"\n        val integersString = \"\"\n        val timestampsString = \"\"\n        val decimalsString = \"\"\n        val allFieldsString = \"COL_NAME,REQUEST,DATAPAKID,PARTNO,RECORD,DOC_NUMBER,S_ORD_ITEM,BARCHKEY,BARCHOFST,SOLD_TO,BBSTNK,MATERIAL,DOC_TYPE,BAUDAT,SALESORG,BORGDNUM,BEARCDTE,RECORDMODE\" \n\n        val fieldLists = IngestAndWrite.loadSchema(keysString,datesString,integersString,timestampsString,decimalsString)(allFieldsString)\n        IngestAndWrite.loadEntity( { StructType(fieldLists._6.map(x => IngestAndWrite.schemaMapper(x, fieldLists._1, fieldLists._2, fieldLists._3, fieldLists._4, fieldLists._5)))}, path, \"csv\")\n    } \n  \n  \n  \n   /*\n    * Method - mergeBofWithZarixsd2\n    * \n    * Parameters - bofDF: DataFrame, zarixsd2DF: DataFrame, orderedColumnsString: String, keys: Array[String]\n    * \n    * Return Type - Try[DataFrame]\n    *\n    * This will pull in Z2 archive table and merge with a the respective BOF. BOF and Zarixsd2 are arguments \n    * \n    * The BEARCDTE column coming from the bof will be kept if not null and if it is Null in bof, the BEARCDTE column in \n    * zarixsd2 will be used.  So, the bof will need to have nulls, and not blanks if it will work.  If the bof will have something \n    * other than nulls, then this function needs to be changed accordingly\n    */\n    def mergeBofWithZarixsd2(bofDF: DataFrame, zarixsd2DF: DataFrame, orderedColumnsString: String, keys: Array[String]): Try[DataFrame] = Try({\n        \n        import spark.implicits._\n        \n        val joinExprs = keys.map{ case (c) => zarixsd2DF(c) <=> bofDF(c) }\n                        .reduce(_ && _) \n         \n        val orderedColumns = orderedColumnsString.split(\",\").toSeq\n        \n        \n        zarixsd2DF.join(bofDF, joinExprs ,\"rightouter\")\n                  .select(bofDF.columns.map(c => if (s\"$c\" == \"BEARCDTE\") coalesce(zarixsd2DF.col(s\"$c\"), bofDF.col(s\"$c\")).alias(\"BEARCDTE\") else bofDF.col(s\"$c\") as c): _*)\n                  .select(orderedColumns.head, orderedColumns.tail: _*)\n                  \n    })   \n    \n    \n    \n    /*This will check for something to do the archive load. The mechanism is TBD */ \n    def triggerZarixsd2Load() : Boolean = {\n        true\n    } \n    \n}",
         "user":"hac4796",
         "dateUpdated":"2017-12-19T17:36:53-0500",
         "config":{
            "editorSetting":{
               "language":"scala",
               "editOnDblClick":false
            },
            "colWidth":12,
            "editorMode":"ace/mode/scala",
            "editorHide":false,
            "results":{
               
            },
            "enabled":true
         },
         "settings":{
            "params":{
               
            },
            "forms":{
            